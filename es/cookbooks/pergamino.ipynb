{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbf394c",
   "metadata": {},
   "source": [
    "# Pergamino {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3e768",
   "metadata": {},
   "source": [
    "The goal of our analysis here in Pergamino is to assess heat risk across the larger area, particularly in urban contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0126ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final steps:\n",
    "\n",
    "# state objectives\n",
    "# assess heat across the department by land cover--just simple distribution\n",
    "# focus in on populated areas: spatially filter, then assess distribution of heat across three classes (rural, periurban, urban)\n",
    "# periurban areas:\n",
    "# map hotspots\n",
    "# quantify risk (total pop, pop in material poverty)\n",
    "# assess drivers\n",
    "# land cover\n",
    "# tree cover\n",
    "# built volume\n",
    "# distance to water\n",
    "# neighbor effects\n",
    "# urban areas:\n",
    "# map hotspots\n",
    "# quantify risk (total pop, pop in material poverty)\n",
    "# assess drivers\n",
    "# land cover\n",
    "# tree cover\n",
    "# built volume\n",
    "# distance to water\n",
    "# neighbor effects\n",
    "# recommendations (drawn heavily from WRI material):\n",
    "# for periurban areas\n",
    "# for urban areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2634b",
   "metadata": {},
   "source": [
    "A SPECIFIC PROBLEM WITH A SPECIFIC SOLUTION IS THAT MANY OF THE PARKS ARE ACTUALLY AMONG THE HOTEST PLACES BECAUSE THEY LACK ADEQUATE TREE COVER. THIS SHOULD BE REMEDIED."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a84928",
   "metadata": {},
   "source": [
    "This WRI article to explain _long-term impact of heat under climate change_: https://www.wri.org/insights/climate-change-effects-cities-15-vs-3-degrees-C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ad724",
   "metadata": {},
   "source": [
    "Other factors, e.g., \"Incidence of arboviruses such as dengue, Zika, West Nile, yellow fever and chikungunya will likely increase worldwide as days with optimal temperatures for disease-carrying mosquitos become more common.\n",
    "\n",
    "Comparing 1.5 degrees C and 3 degrees C of warming, the average increase in peak arbovirus-transmission days globally is 6 days. But the picture is much more complicated than the global average.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31387a5c",
   "metadata": {},
   "source": [
    "\"Arboviruses are of particularly urgent concern in Latin America. Brazil is already experiencing a dengue crisis, and at 3 degrees C of warming, 11 of its largest cities could see high arbovirus risk for at least six months of the year. Rio de Janeiro may see significantly more illness if global warming reaches 3 degrees C, as the expected number of peak arbovirus days increases by 71%, from 69 to 118 days per year.\n",
    "\n",
    "As temperatures in Rio de Janeiro become more hospitable to dengue-carrying mosquitos, the city is investing in improving the availability of dengue vaccines and controlling the mosquitos themselves. Community health workers crisscross the city, hunting for places where standing water accumulates and mosquitos can breed. And Rio de Janeiro is using another disease — Wolbachia, a bacterium that infects insects — to inhibit dengue transmission in infected mosquitos. Rio de Janeiro and five other Brazilian cities release Wolbachia-infected mosquitos by the tens of thousands. These mosquitos behave no differently from uninfected mosquitos, but they harbor less dengue virus and infect other mosquitos. A similar Wolbachia program in Yogyakarta, Indonesia resulted in a 77% reduction of dengue incidence.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9369e8a",
   "metadata": {},
   "source": [
    "Solutions:\n",
    "- Cooling infrastructure (https://www.wri.org/outcomes/indian-cities-pioneer-nature-based-solutions-and-inspire-national-climate-action, https://www.wri.org/insights/urban-heat-effect-solutions)\n",
    "- NBS (https://www.wri.org/insights/what-exactly-are-nature-based-solutions)\n",
    "- Heat action plan (https://onebillionresilient.org/hot-cities-chilled-economies-new-delhi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85943903",
   "metadata": {},
   "source": [
    "Oooooh, add a quick analysis of LST versus % poverty per census tract! It's clearly correlated, which means this is an equity issue, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b13812",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827d6fb",
   "metadata": {},
   "source": [
    "## Exploratory data analysis:\n",
    "- Map LST across the department (add basic summary stats to map: min, mean, median, max)\n",
    "- Map Land cover across deparmtnet (simple legend)\n",
    "- Quick summary of LST by land cover type--maybe density plots by land cover class? or box and whisker? Which will be easier for municipal govt to interpret?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2060a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "import pystac_client\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from odc.stac import configure_rio, stac_load\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "from io import BytesIO\n",
    "from owslib.wfs import WebFeatureService\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from dask.diagnostics import ProgressBar\n",
    "import rioxarray as rio\n",
    "\n",
    "\n",
    "USE_CRS = \"EPSG:5347\"\n",
    "CRS_WGS84 = \"EPSG:4326\"\n",
    "\n",
    "RUTA_BASE = Path(\"/home/nissim/Documents/dev/datos-escala-humana/\")\n",
    "RUTA_DATOS = RUTA_BASE / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499fe66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wfs_to_gdf(\n",
    "    wfs_url: str, layer_name: str, srs: str = \"EPSG:4326\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Descarga una capa WFS y la devuelve como GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        wfs_url (str): URL del servicio WFS.\n",
    "        layer_name (str): Nombre de la capa (typename).\n",
    "        srs (str): Código EPSG del sistema de referencia de coordenadas.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Capa descargada como GeoDataFrame.\n",
    "    \"\"\"\n",
    "    wfs = WebFeatureService(url=wfs_url, version=\"2.0.0\")\n",
    "    response = wfs.getfeature(typename=layer_name, srsname=srs)\n",
    "    gdf = gpd.read_file(BytesIO(response.read()))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f56b667",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='wms.ign.gob.ar', port=443): Read timed out. (read timeout=30)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/http/client.py:1423\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1423\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.2/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/util/util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='wms.ign.gob.ar', port=443): Read timed out. (read timeout=30)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     munis = gpd.read_file(RUTA_DATOS / \u001b[33m\"\u001b[39m\u001b[33mmunicipios.geojson\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     munis = \u001b[43mwfs_to_gdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwfs_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mign:municipio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrs\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEPSG:4326\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     munis.to_file(RUTA_DATOS / \u001b[33m\"\u001b[39m\u001b[33mmunicipios.geojson\u001b[39m\u001b[33m\"\u001b[39m, driver=\u001b[33m\"\u001b[39m\u001b[33mGeoJSON\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m pergamino = munis[munis[\u001b[33m\"\u001b[39m\u001b[33mnam\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mPergamino\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mwfs_to_gdf\u001b[39m\u001b[34m(wfs_url, layer_name, srs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwfs_to_gdf\u001b[39m(\n\u001b[32m      2\u001b[39m     wfs_url: \u001b[38;5;28mstr\u001b[39m, layer_name: \u001b[38;5;28mstr\u001b[39m, srs: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mEPSG:4326\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m ) -> gpd.GeoDataFrame:\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    Descarga una capa WFS y la devuelve como GeoDataFrame.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m        gpd.GeoDataFrame: Capa descargada como GeoDataFrame.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     wfs = \u001b[43mWebFeatureService\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwfs_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     response = wfs.getfeature(typename=layer_name, srsname=srs)\n\u001b[32m     17\u001b[39m     gdf = gpd.read_file(BytesIO(response.read()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/owslib/wfs.py:56\u001b[39m, in \u001b[36mWebFeatureService\u001b[39m\u001b[34m(url, version, xml, parse_remote_metadata, timeout, username, password, headers, auth)\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wfs110.WebFeatureService_1_1_0(\n\u001b[32m     53\u001b[39m         clean_url, version, xml, parse_remote_metadata,\n\u001b[32m     54\u001b[39m         timeout=timeout, headers=headers, auth=auth)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m version \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33m2.0\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m2.0.0\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwfs200\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWebFeatureService_2_0_0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclean_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_remote_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/owslib/feature/wfs200.py:86\u001b[39m, in \u001b[36mWebFeatureService_2_0_0.__init__\u001b[39m\u001b[34m(self, url, version, xml, parse_remote_metadata, timeout, headers, username, password, auth)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m._capabilities = reader.readString(xml)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28mself\u001b[39m._capabilities = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m._buildMetadata(parse_remote_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/owslib/feature/common.py:54\u001b[39m, in \u001b[36mWFSCapabilitiesReader.read\u001b[39m\u001b[34m(self, url, timeout)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get and parse a WFS capabilities document, returning an\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03minstance of WFSCapabilitiesInfoset\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m    A timeout value (in seconds) for the request.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m request = \u001b[38;5;28mself\u001b[39m.capabilities_url(url)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m u = \u001b[43mopenURL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m getXMLTree(u)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/owslib/util.py:207\u001b[39m, in \u001b[36mopenURL\u001b[39m\u001b[34m(url_base, data, method, cookies, username, password, timeout, headers, verify, cert, auth)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cookies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    205\u001b[39m     rkwargs[\u001b[33m'\u001b[39m\u001b[33mcookies\u001b[39m\u001b[33m'\u001b[39m] = cookies\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m req = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m req.status_code \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m400\u001b[39m, \u001b[32m401\u001b[39m, \u001b[32m403\u001b[39m]:\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(req.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dev/datos-escala-humana/.venv/lib/python3.12/site-packages/requests/adapters.py:690\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    688\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request=request)\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='wms.ign.gob.ar', port=443): Read timed out. (read timeout=30)"
     ]
    }
   ],
   "source": [
    "base_url = \"https://wms.ign.gob.ar/geoserver/ign/ows\"\n",
    "\n",
    "if (RUTA_DATOS / \"municipios.geojson\").exists():\n",
    "    munis = gpd.read_file(RUTA_DATOS / \"municipios.geojson\")\n",
    "else:\n",
    "    munis = wfs_to_gdf(wfs_url=base_url, layer_name=\"ign:municipio\", srs=\"EPSG:4326\")\n",
    "    munis.to_file(RUTA_DATOS / \"municipios.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "pergamino = munis[munis[\"nam\"] == \"Pergamino\"]\n",
    "pergamino = pergamino.to_crs(USE_CRS)\n",
    "\n",
    "bbox_pergamino_4326 = pergamino.to_crs(CRS_WGS84).total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d452b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Planetary Computer\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    processes=True,  # Critical: use processes, not threads\n",
    "    n_workers=8,  # Use half your cores for workers\n",
    "    threads_per_worker=2,  # 2 threads per worker = 16 total\n",
    "    memory_limit=\"6GB\",  # 8 workers × 6GB = 48GB, leaves headroom\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "configure_rio(cloud_defaults=True, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1261753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for last 5 years\n",
    "bbox = bbox_pergamino_4326\n",
    "datetime = \"2020-01-01/2025-01-01\"  # Last 5 years\n",
    "cloudy_less_than = 20  # You might need to be less strict for 5 years of data\n",
    "\n",
    "query = catalog.search(\n",
    "    collections=[\"landsat-c2-l2\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": cloudy_less_than}},\n",
    ")\n",
    "\n",
    "items = list(query.items())\n",
    "print(f\"Found: {len(items):d} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file path (same directory as notebook)\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.path.dirname(os.path.abspath(\"pergamino.ipynb\")))\n",
    "lst_tif_path = notebook_dir / \"LST_max_pergamino.tif\"\n",
    "\n",
    "# Check if file exists, otherwise compute\n",
    "if lst_tif_path.exists():\n",
    "    print(f\"Loading existing LST data from {lst_tif_path}\")\n",
    "    LST_max_computed = rio.open_rasterio(lst_tif_path)\n",
    "    # Squeeze out any extra dimensions if needed\n",
    "    if len(LST_max_computed.dims) > 2:\n",
    "        LST_max_computed = LST_max_computed.squeeze()\n",
    "else:\n",
    "    print(\"Computing LST from scratch...\")\n",
    "    # Load only thermal band and QA band\n",
    "    crs = \"epsg:3857\"\n",
    "    yy = stac_load(\n",
    "        items,\n",
    "        bands=(\"lwir11\", \"qa_pixel\"),\n",
    "        crs=crs,\n",
    "        resolution=30,\n",
    "        chunks={\"time\": 10},  # Process in chunks of 10 time steps\n",
    "        groupby=\"landsat:scene_id\",\n",
    "        bbox=bbox_pergamino_4326,\n",
    "    )\n",
    "\n",
    "    # Create cloud mask function\n",
    "    def create_cloud_mask(qa_band):\n",
    "        \"\"\"Mask clouds and cloud shadows\"\"\"\n",
    "        # Bit 3: Cloud, Bit 4: Cloud Shadow\n",
    "        cloud_mask = (qa_band & 0b11000) == 0\n",
    "        return cloud_mask\n",
    "\n",
    "    # Apply cloud mask\n",
    "    yy_masked = yy.where(create_cloud_mask(yy.qa_pixel))\n",
    "\n",
    "    # Convert thermal band to LST in Celsius\n",
    "    LST_kelvin = yy_masked[\"lwir11\"] * 0.00341802 + 149.0\n",
    "    LST_celsius = LST_kelvin - 273.15\n",
    "\n",
    "    # Calculate maximum LST per pixel over all time steps\n",
    "    LST_max = LST_celsius.max(dim=\"time\")\n",
    "\n",
    "    # Before compute()\n",
    "    print(\"Computing maximum LST...\")\n",
    "    print(f\"Dashboard: {client.dashboard_link}\")\n",
    "    with ProgressBar():\n",
    "        LST_max_computed = LST_max.compute()\n",
    "\n",
    "    # Save to file\n",
    "    print(f\"Saving LST data to {lst_tif_path}\")\n",
    "    LST_max_computed.rio.to_raster(lst_tif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "LST_max_computed.plot.imshow(\n",
    "    ax=ax,\n",
    "    cmap=\"afmhot\",\n",
    "    cbar_kwargs={\"label\": \"Maximum LST (°C)\"},\n",
    "    robust=True,  # Use 2nd and 98th percentiles for color scaling\n",
    ")\n",
    "ax.set_title(\"Maximum Land Surface Temperature (2020-2025, Cloud Masked)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to file\n",
    "# LST_max_computed.rio.to_raster(RUTA_DATOS / \"LST_max_5years.tif\")\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\nMaximum LST Statistics:\")\n",
    "print(f\"  Overall max: {float(LST_max_computed.max().values):.2f}°C\")\n",
    "print(f\"  Overall min: {float(LST_max_computed.min().values):.2f}°C\")\n",
    "print(f\"  Mean of maximums: {float(LST_max_computed.mean().values):.2f}°C\")\n",
    "print(f\"  Median of maximums: {float(LST_max_computed.median().values):.2f}°C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cloud GeoTIFF clipped to your GeoDataFrame\n",
    "suelo_2022_ruta = \"https://storage.googleapis.com/mapbiomas-public/initiatives/argentina/collection-1/coverage/argentina_coverage_2022.tif\"\n",
    "\n",
    "# 1. Use your existing Dask cluster for parallel reads\n",
    "suelo_2022 = rio.open_rasterio(\n",
    "    suelo_2022_ruta,\n",
    "    chunks={\"x\": 2048, \"y\": 2048},  # Larger chunks = fewer tasks, faster\n",
    "    lock=False,  # Critical: allows parallel HTTP reads from GCS\n",
    ")\n",
    "\n",
    "# For the clip/compute operation\n",
    "with ProgressBar():\n",
    "    # 2. Clip efficiently\n",
    "    suelo_2022 = suelo_2022.rio.clip(\n",
    "        pergamino.geometry.values,\n",
    "        pergamino.crs,\n",
    "        from_disk=True,\n",
    "        all_touched=False,  # Only pixels with centers inside\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the detailed categories and colors\n",
    "categories_detailed = {\n",
    "    \"Leñosa cerrada\": {\"ids\": [3], \"color\": \"#1f8d49\"},\n",
    "    \"Leñosa abierta\": {\"ids\": [4], \"color\": \"#7dc975\"},\n",
    "    \"Leñosa dispersa\": {\"ids\": [45], \"color\": \"#807a40\"},\n",
    "    \"Leñosa inundable\": {\"ids\": [6], \"color\": \"#026975\"},\n",
    "    \"Vegetación no leñosa inundable\": {\"ids\": [11], \"color\": \"#519799\"},\n",
    "    \"Pastizal\": {\"ids\": [12], \"color\": \"#d6bc74\"},\n",
    "    \"Estepa\": {\"ids\": [63], \"color\": \"#ebf8b5\"},\n",
    "    \"Pastura\": {\"ids\": [15], \"color\": \"#edde8e\"},\n",
    "    \"Agricultura\": {\"ids\": [18], \"color\": \"#e974ed\"},\n",
    "    \"Plantación forestal\": {\"ids\": [9], \"color\": \"#7a5900\"},\n",
    "    \"Cultivo arbustivo\": {\"ids\": [36], \"color\": \"#d082de\"},\n",
    "    \"Mosaico agropecuario\": {\"ids\": [21], \"color\": \"#ffefc3\"},\n",
    "    \"Área sin vegetación\": {\"ids\": [22], \"color\": \"#d4271e\"},\n",
    "    \"Río, lago u océano\": {\"ids\": [33], \"color\": \"#2532e4\"},\n",
    "    \"Hielo y nieve en superficie\": {\"ids\": [34], \"color\": \"#93dfe6\"},\n",
    "    \"No observado\": {\"ids\": [27], \"color\": \"#ffffff\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Create colormap from detailed categories\n",
    "colors = {}\n",
    "for category, info in categories_detailed.items():\n",
    "    for pixel_id in info[\"ids\"]:\n",
    "        colors[pixel_id] = info[\"color\"]\n",
    "\n",
    "cmap = mcolors.ListedColormap([colors[i] for i in sorted(colors.keys())])\n",
    "bounds = sorted(colors.keys())\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot the raster\n",
    "im = suelo_2022.plot(ax=ax, cmap=cmap, norm=norm, add_colorbar=False)\n",
    "\n",
    "# Create custom legend from detailed categories\n",
    "legend_labels = {}\n",
    "for category, info in categories_detailed.items():\n",
    "    legend_labels[info[\"color\"]] = category\n",
    "\n",
    "# Add legend below the map\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor=color, label=label)\n",
    "    for color, label in legend_labels.items()\n",
    "]\n",
    "ax.legend(\n",
    "    handles=legend_elements,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.1),\n",
    "    ncol=3,\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "plt.title(\"Cobertura del Suelo - MapBiomas 2022\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reproject land cover to match LST grid (if needed)\n",
    "suelo_aligned = suelo_2022.rio.reproject_match(LST_max_computed)\n",
    "\n",
    "# 2. Flatten both arrays and create DataFrame\n",
    "lst_flat = LST_max_computed.values.flatten()\n",
    "landcover_flat = suelo_aligned.values.flatten()\n",
    "\n",
    "df = pd.DataFrame({\"LST\": lst_flat, \"LandCover\": landcover_flat})\n",
    "\n",
    "# 3. Filter out invalid values\n",
    "df_clean = df[\n",
    "    (df[\"LandCover\"] > 0)  # Remove 0\n",
    "    & (df[\"LandCover\"] < 255)  # Remove 255\n",
    "    & (~df[\"LandCover\"].isna())  # Remove NaN\n",
    "    & (~df[\"LST\"].isna())  # Remove LST NaN\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from IDs to category names\n",
    "id_to_name = {}\n",
    "for category, details in categories_detailed.items():\n",
    "    for id_val in details[\"ids\"]:\n",
    "        id_to_name[id_val] = category\n",
    "\n",
    "# Add readable names to dataframe\n",
    "df_clean[\"Category\"] = df_clean[\"LandCover\"].astype(int).map(id_to_name)\n",
    "\n",
    "# Sort by median LST for better visualization\n",
    "category_order = (\n",
    "    df_clean.groupby(\"Category\")[\"LST\"].median().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "# Box plot with sorted categories\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "df_clean.boxplot(\n",
    "    column=\"LST\",\n",
    "    by=\"Category\",\n",
    "    ax=ax,\n",
    "    showfliers=False,  # Hide outliers for cleaner view\n",
    "    positions=range(len(category_order)),\n",
    "    patch_artist=True,  # Allows coloring boxes\n",
    ")\n",
    "\n",
    "# Set x-axis with sorted order\n",
    "ax.set_xticklabels(category_order, rotation=45, ha=\"right\")\n",
    "ax.set_xlabel(\"Categoría de Cobertura del Suelo\", fontsize=12)\n",
    "ax.set_ylabel(\"LST Máxima (°C)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Distribución de Temperatura Superficial por Cobertura del Suelo\\n(Máximo 2020-2025, Pergamino)\",\n",
    "    fontsize=14,\n",
    "    pad=20,\n",
    ")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.suptitle(\"\")  # Remove default pandas title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84750a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics with Spanish names\n",
    "summary = (\n",
    "    df_clean.groupby(\"Category\")[\"LST\"]\n",
    "    .agg(\n",
    "        [\n",
    "            (\"Píxeles\", \"count\"),\n",
    "            (\"Media\", \"mean\"),\n",
    "            (\"Mediana\", \"median\"),\n",
    "            (\"Desv. Est.\", \"std\"),\n",
    "            (\"Mín\", \"min\"),\n",
    "            (\"Máx\", \"max\"),\n",
    "        ]\n",
    "    )\n",
    "    .round(2)\n",
    "    .sort_values(\"Mediana\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nEstadísticas de LST por Categoría:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e759510",
   "metadata": {},
   "source": [
    "## Populated areas:\n",
    "- Map populated areas\n",
    "\n",
    "Question: is it worth including summary stats of population, buildings per populated area? Or is that unnecessary? Maybe it's something that I jsut *gesture* at--indicate in teh comments that one _could_ do it, but we will not because it's out of scope. But link to how to do it.\n",
    "\n",
    "- Descriptive stats--more distributions of heat per area. WHich ones are hottest on average? (It'll be urban and peri-urban)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef0339",
   "metadata": {},
   "source": [
    "## Periurban areas\n",
    "\n",
    "### EDA\n",
    "- Plot distribution\n",
    "- Classify quartiles\n",
    "- Map hotspots (top 25%)\n",
    "- Zoom in particularly on industrial parks, for example\n",
    "- Assess by land cover type (tukey, etc)\n",
    "- Assess by tree canopy cover (correlation? box plots? this might involve some log transformations to make it intelligible)\n",
    "- Assess by built volume\n",
    "- Assess by distance to water\n",
    "\n",
    "### Modeling\n",
    "- Build model including neighborhood factors\n",
    "- Model + interpret. Include spatial cross-validation, SHAP assessment.\n",
    "     - How reproducible is this? I wonder...\n",
    "- Explain in clear langauge what it means for policy implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fac4c",
   "metadata": {},
   "source": [
    "## Urban core\n",
    "\n",
    "### EDA\n",
    "- Plot distribution\n",
    "- Classify quartiles\n",
    "- Map hotspots (top 25%)\n",
    "- Zoom in particularly on industrial parks, for example\n",
    "- Assess by land cover type (tukey, etc)\n",
    "- Assess by tree canopy cover (correlation? box plots? this might involve some log transformations to make it intelligible)\n",
    "- Assess by built volume\n",
    "- Assess by distance to water\n",
    "\n",
    "### Modeling\n",
    "- Build model including neighborhood factors\n",
    "- Model + interpret. Include spatial cross-validation, SHAP assessment.\n",
    "     - How reproducible is this? I wonder...\n",
    "- Explain in clear langauge what it means for policy implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11988b",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "- Draw neavily from WRI material\n",
    "- Split for periurban versus urban areas (what's feasible, desirable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "import pystac_client\n",
    "import matplotlib.pyplot as plt\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from odc.stac import configure_rio, stac_load\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import colors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import rioxarray as rio\n",
    "\n",
    "\n",
    "USE_CRS = \"EPSG:5347\"\n",
    "CRS_WGS84 = \"EPSG:4326\"\n",
    "\n",
    "RUTA_BASE = Path(\"/home/nissim/Documents/dev/datos-escala-humana/\")\n",
    "RUTA_DATOS = RUTA_BASE / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0517c1",
   "metadata": {},
   "source": [
    "So the first thing we're going to do is simply load our municipal boundaries from the Instituto Geográfico Nacional, their open data portal as a web feature service, so we'll download those or we'll load those directly and save them as a local GeoJSON file um.\n",
    "And we're going to take their bounding box which we're going to use for our spatial queries um for uh.\n",
    "And we're going to use a lot of data reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wfs_to_gdf(\n",
    "    wfs_url: str, layer_name: str, srs: str = \"EPSG:4326\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Descarga una capa WFS y la devuelve como GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        wfs_url (str): URL del servicio WFS.\n",
    "        layer_name (str): Nombre de la capa (typename).\n",
    "        srs (str): Código EPSG del sistema de referencia de coordenadas.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Capa descargada como GeoDataFrame.\n",
    "    \"\"\"\n",
    "    wfs = WebFeatureService(url=wfs_url, version=\"2.0.0\")\n",
    "    response = wfs.getfeature(typename=layer_name, srsname=srs)\n",
    "    gdf = gpd.read_file(BytesIO(response.read()))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ce41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://wms.ign.gob.ar/geoserver/ign/ows\"\n",
    "\n",
    "if (RUTA_DATOS / \"municipios.geojson\").exists():\n",
    "    munis = gpd.read_file(RUTA_DATOS / \"municipios.geojson\")\n",
    "else:\n",
    "    munis = wfs_to_gdf(wfs_url=base_url, layer_name=\"ign:municipio\", srs=\"EPSG:4326\")\n",
    "\n",
    "pergamino = munis[munis[\"nam\"] == \"Pergamino\"]\n",
    "pergamino = pergamino.to_crs(USE_CRS)\n",
    "\n",
    "bbox_pergamino_4326 = pergamino.to_crs(CRS_WGS84).total_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ad045",
   "metadata": {},
   "source": [
    "The next thing we're going to do is use the pystack client library to connect the microsoft planetary computer microsoft planetary computer has uh hosts a lot of uh open satellite data and in this case we're going to use it to get landsat data which is a united states geological service satellite um which uh we're going to use the thermal band from um from um that satellite to um to pull in our uh land surface temperature data um so we'll connect to that satellite and we're going to set up um we're going to use dask actually to set up a local cluster for parallel processing um we're going to be using um a number of different workers to efficiently download these data.\n",
    "So we'll set up a cluster that takes advantage of the multiple cores on our computer to be able to run in parallel uh efficiently and more efficiently process uh the data that we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68710772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Planetary Computer\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(\n",
    "    processes=True,  # Critical: use processes, not threads\n",
    "    n_workers=8,  # Use half your cores for workers\n",
    "    threads_per_worker=2,  # 2 threads per worker = 16 total\n",
    "    memory_limit=\"6GB\",  # 8 workers × 6GB = 48GB, leaves headroom\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(f\"Dask dashboard: {client.dashboard_link}\")\n",
    "configure_rio(cloud_defaults=True, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0aad6c",
   "metadata": {},
   "source": [
    "Next up we're going to pass a query to pystack client we're going to search through the landsat catalog using our area of interest for a date range from the beginning of 2020 through the beginning of 2025 so we're going to look through a five-year date range 2020 2021 2022 2023 2024 we're going to filter out areas with cloud cover um and we're going to process all these data sets now there are a lot of these data sets so this is which is why we're going to be using dask and what we're going to do is um process them um to uh calculate land surface temperature um and calculate the max land surface temperature per pixel in the last five years so the value of the pixel will be the max land surface temperature value of that pixel in the last five years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca78b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for last 5 years\n",
    "bbox = bbox_pergamino_4326\n",
    "datetime = \"2020-01-01/2025-01-01\"  # Last 5 years\n",
    "cloudy_less_than = 20  # You might need to be less strict for 5 years of data\n",
    "\n",
    "query = catalog.search(\n",
    "    collections=[\"landsat-c2-l2\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": cloudy_less_than}},\n",
    ")\n",
    "\n",
    "items = list(query.items())\n",
    "print(f\"Found: {len(items):d} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file path (same directory as notebook)\n",
    "import os\n",
    "\n",
    "notebook_dir = Path(os.path.dirname(os.path.abspath(\"pergamino.ipynb\")))\n",
    "lst_tif_path = notebook_dir / \"LST_max_pergamino.tif\"\n",
    "\n",
    "# Check if file exists, otherwise compute\n",
    "if lst_tif_path.exists():\n",
    "    print(f\"Loading existing LST data from {lst_tif_path}\")\n",
    "    LST_max_computed = rio.open_rasterio(lst_tif_path)\n",
    "    # Squeeze out any extra dimensions if needed\n",
    "    if len(LST_max_computed.dims) > 2:\n",
    "        LST_max_computed = LST_max_computed.squeeze()\n",
    "else:\n",
    "    print(\"Computing LST from scratch...\")\n",
    "    # Load only thermal band and QA band\n",
    "    crs = \"epsg:3857\"\n",
    "    yy = stac_load(\n",
    "        items,\n",
    "        bands=(\"lwir11\", \"qa_pixel\"),\n",
    "        crs=crs,\n",
    "        resolution=30,\n",
    "        chunks={\"time\": 10},  # Process in chunks of 10 time steps\n",
    "        groupby=\"landsat:scene_id\",\n",
    "        bbox=bbox_pergamino_4326,\n",
    "    )\n",
    "\n",
    "    # Create cloud mask function\n",
    "    def create_cloud_mask(qa_band):\n",
    "        \"\"\"Mask clouds and cloud shadows\"\"\"\n",
    "        # Bit 3: Cloud, Bit 4: Cloud Shadow\n",
    "        cloud_mask = (qa_band & 0b11000) == 0\n",
    "        return cloud_mask\n",
    "\n",
    "    # Apply cloud mask\n",
    "    yy_masked = yy.where(create_cloud_mask(yy.qa_pixel))\n",
    "\n",
    "    # Convert thermal band to LST in Celsius\n",
    "    LST_kelvin = yy_masked[\"lwir11\"] * 0.00341802 + 149.0\n",
    "    LST_celsius = LST_kelvin - 273.15\n",
    "\n",
    "    # Calculate maximum LST per pixel over all time steps\n",
    "    LST_max = LST_celsius.max(dim=\"time\")\n",
    "\n",
    "    # Before compute()\n",
    "    print(\"Computing maximum LST...\")\n",
    "    print(f\"Dashboard: {client.dashboard_link}\")\n",
    "    with ProgressBar():\n",
    "        LST_max_computed = LST_max.compute()\n",
    "\n",
    "    # Save to file\n",
    "    print(f\"Saving LST data to {lst_tif_path}\")\n",
    "    LST_max_computed.rio.to_raster(lst_tif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc38257",
   "metadata": {},
   "source": [
    "Once we've done that we're going to um plot these data and we're going to see something interesting which is something that reflects land surface temperature and how it operates one land surface temperature these values are going to look insanely high um and they are because land surface temperature is the temperature of the ground itself which can be a lot hotter than the air um can absorb heat a lot and is is not connected to the temperature that we feel i mean it's not directly connected to the temperature we feel objectively to um to evaluate um.\n",
    "The temperature that people experience we should be using things like universal thermal comfort or mean radiant temperature which are much better indicators of how people feel temperature but those are much more complicated to calculate so one of the approaches that we're taking here is to simply um is to to calculate these things in a way that's like less precise but is still reflective of how hot things are um and still gives us an idea of where we need to focus our energy and that's one of the major sort of theses of this project that we can actually have a lot of value with um available data sets rather than holding out for perfect data sets we can still generate a lot of really useful information for decision makers with data sets maybe not ideal but are good enough to inform the decision we want to make in this case land surface temperature is one of these things.\n",
    "So we'll calculate land surface temperature here and then we'll calculate some quick summary statistics to just get a sense of what the distribution is and we'll see based on it of course that the distribution is not uh returns temperature values that are not that are very high in many cases right um but these are not air temperature these are land surface temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7286c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "LST_max_computed.plot.imshow(\n",
    "    ax=ax,\n",
    "    cmap=\"afmhot\",\n",
    "    cbar_kwargs={\"label\": \"Maximum LST (°C)\"},\n",
    "    robust=True,  # Use 2nd and 98th percentiles for color scaling\n",
    ")\n",
    "ax.set_title(\"Maximum Land Surface Temperature (2020-2025, Cloud Masked)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd58b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to file\n",
    "# LST_max_computed.rio.to_raster(RUTA_DATOS / \"LST_max_5years.tif\")\n",
    "\n",
    "# Print some statistics\n",
    "print(\"\\nMaximum LST Statistics:\")\n",
    "print(f\"  Overall max: {float(LST_max_computed.max().values):.2f}°C\")\n",
    "print(f\"  Overall min: {float(LST_max_computed.min().values):.2f}°C\")\n",
    "print(f\"  Mean of maximums: {float(LST_max_computed.mean().values):.2f}°C\")\n",
    "print(f\"  Median of maximums: {float(LST_max_computed.median().values):.2f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd5700",
   "metadata": {},
   "source": [
    "## Cobertura del suelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ba4e7",
   "metadata": {},
   "source": [
    "Next we're going to load um land cover data for all of argentina from biomass um these are 2022 which is the most recent available year obviously 2025 data would be ideal but those aren't out yet um we're going to load these they're 30 meter resolution derived from actually landsat which is perfect because that'll make it really easy to align them with our our other landsat our heat data we're going to import these also using gas for efficiency's sake um we're going to quickly map them so we can see them uh with the color um that sort of has been given to us from the map biomass website and we'll just take a look at them across the province um and as you can see you know the areas that are showing up as hottest on our map are actually um a lot of the agricultural areas um and the natural areas and the urban areas are slightly cooler um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cloud GeoTIFF clipped to your GeoDataFrame\n",
    "suelo_2022_ruta = \"https://storage.googleapis.com/mapbiomas-public/initiatives/argentina/collection-1/coverage/argentina_coverage_2022.tif\"\n",
    "\n",
    "# 1. Use your existing Dask cluster for parallel reads\n",
    "suelo_2022 = rio.open_rasterio(\n",
    "    suelo_2022_ruta,\n",
    "    chunks={\"x\": 2048, \"y\": 2048},  # Larger chunks = fewer tasks, faster\n",
    "    lock=False,  # Critical: allows parallel HTTP reads from GCS\n",
    ")\n",
    "\n",
    "# For the clip/compute operation\n",
    "with ProgressBar():\n",
    "    # 2. Clip efficiently\n",
    "    suelo_2022 = suelo_2022.rio.clip(\n",
    "        pergamino.geometry.values,\n",
    "        pergamino.crs,\n",
    "        from_disk=True,\n",
    "        all_touched=False,  # Only pixels with centers inside\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af7fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the detailed categories and colors\n",
    "categories_detailed = {\n",
    "    \"Leñosa cerrada\": {\"ids\": [3], \"color\": \"#1f8d49\"},\n",
    "    \"Leñosa abierta\": {\"ids\": [4], \"color\": \"#7dc975\"},\n",
    "    \"Leñosa dispersa\": {\"ids\": [45], \"color\": \"#807a40\"},\n",
    "    \"Leñosa inundable\": {\"ids\": [6], \"color\": \"#026975\"},\n",
    "    \"Vegetación no leñosa inundable\": {\"ids\": [11], \"color\": \"#519799\"},\n",
    "    \"Pastizal\": {\"ids\": [12], \"color\": \"#d6bc74\"},\n",
    "    \"Estepa\": {\"ids\": [63], \"color\": \"#ebf8b5\"},\n",
    "    \"Pastura\": {\"ids\": [15], \"color\": \"#edde8e\"},\n",
    "    \"Agricultura\": {\"ids\": [18], \"color\": \"#e974ed\"},\n",
    "    \"Plantación forestal\": {\"ids\": [9], \"color\": \"#7a5900\"},\n",
    "    \"Cultivo arbustivo\": {\"ids\": [36], \"color\": \"#d082de\"},\n",
    "    \"Mosaico agropecuario\": {\"ids\": [21], \"color\": \"#ffefc3\"},\n",
    "    \"Área sin vegetación\": {\"ids\": [22], \"color\": \"#d4271e\"},\n",
    "    \"Río, lago u océano\": {\"ids\": [33], \"color\": \"#2532e4\"},\n",
    "    \"Hielo y nieve en superficie\": {\"ids\": [34], \"color\": \"#93dfe6\"},\n",
    "    \"No observado\": {\"ids\": [27], \"color\": \"#ffffff\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5abfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Create colormap from detailed categories\n",
    "colors = {}\n",
    "for category, info in categories_detailed.items():\n",
    "    for pixel_id in info[\"ids\"]:\n",
    "        colors[pixel_id] = info[\"color\"]\n",
    "\n",
    "cmap = mcolors.ListedColormap([colors[i] for i in sorted(colors.keys())])\n",
    "bounds = sorted(colors.keys())\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Plot the raster\n",
    "im = suelo_2022.plot(ax=ax, cmap=cmap, norm=norm, add_colorbar=False)\n",
    "\n",
    "# Create custom legend from detailed categories\n",
    "legend_labels = {}\n",
    "for category, info in categories_detailed.items():\n",
    "    legend_labels[info[\"color\"]] = category\n",
    "\n",
    "# Add legend below the map\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor=color, label=label)\n",
    "    for color, label in legend_labels.items()\n",
    "]\n",
    "ax.legend(\n",
    "    handles=legend_elements,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.1),\n",
    "    ncol=3,\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "plt.title(\"Cobertura del Suelo - MapBiomas 2022\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61239ee0",
   "metadata": {},
   "source": [
    "## Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad85ee",
   "metadata": {},
   "source": [
    "Now we'll get into our analysis in our analysis what we're going to do is actually um assess how land surface temperature is distributed across the different um the different land cover classes right so how temperature maps on to uh.\n",
    "How temperature maps onto land cover type what we'll see from this is that um you know um natural land cover like forest and wooded area or water is much cooler on average than agriculture and that urban areas which are areas that vegetation are on the higher to middle end of that spectrum so not as hot as the total bare ground because bare ground has no tree cover no building shade no water to cool it off but um their urban areas are also not as cool as forested areas or a lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reproject land cover to match LST grid (if needed)\n",
    "suelo_aligned = suelo_2022.rio.reproject_match(LST_max_computed)\n",
    "\n",
    "# 2. Flatten both arrays and create DataFrame\n",
    "lst_flat = LST_max_computed.values.flatten()\n",
    "landcover_flat = suelo_aligned.values.flatten()\n",
    "\n",
    "df = pd.DataFrame({\"LST\": lst_flat, \"LandCover\": landcover_flat})\n",
    "\n",
    "# 3. Filter out invalid values\n",
    "df_clean = df[\n",
    "    (df[\"LandCover\"] > 0)  # Remove 0\n",
    "    & (df[\"LandCover\"] < 255)  # Remove 255\n",
    "    & (~df[\"LandCover\"].isna())  # Remove NaN\n",
    "    & (~df[\"LST\"].isna())  # Remove LST NaN\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from IDs to category names\n",
    "id_to_name = {}\n",
    "for category, details in categories_detailed.items():\n",
    "    for id_val in details[\"ids\"]:\n",
    "        id_to_name[id_val] = category\n",
    "\n",
    "# Add readable names to dataframe\n",
    "df_clean[\"Category\"] = df_clean[\"LandCover\"].astype(int).map(id_to_name)\n",
    "\n",
    "# Sort by median LST for better visualization\n",
    "category_order = (\n",
    "    df_clean.groupby(\"Category\")[\"LST\"].median().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "# Box plot with sorted categories\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "df_clean.boxplot(\n",
    "    column=\"LST\",\n",
    "    by=\"Category\",\n",
    "    ax=ax,\n",
    "    showfliers=False,  # Hide outliers for cleaner view\n",
    "    positions=range(len(category_order)),\n",
    "    patch_artist=True,  # Allows coloring boxes\n",
    ")\n",
    "\n",
    "# Set x-axis with sorted order\n",
    "ax.set_xticklabels(category_order, rotation=45, ha=\"right\")\n",
    "ax.set_xlabel(\"Categoría de Cobertura del Suelo\", fontsize=12)\n",
    "ax.set_ylabel(\"LST Máxima (°C)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Distribución de Temperatura Superficial por Cobertura del Suelo\\n(Máximo 2020-2025, Pergamino)\",\n",
    "    fontsize=14,\n",
    "    pad=20,\n",
    ")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.suptitle(\"\")  # Remove default pandas title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics with Spanish names\n",
    "summary = (\n",
    "    df_clean.groupby(\"Category\")[\"LST\"]\n",
    "    .agg(\n",
    "        [\n",
    "            (\"Píxeles\", \"count\"),\n",
    "            (\"Media\", \"mean\"),\n",
    "            (\"Mediana\", \"median\"),\n",
    "            (\"Desv. Est.\", \"std\"),\n",
    "            (\"Mín\", \"min\"),\n",
    "            (\"Máx\", \"max\"),\n",
    "        ]\n",
    "    )\n",
    "    .round(2)\n",
    "    .sort_values(\"Mediana\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nEstadísticas de LST por Categoría:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aad499",
   "metadata": {},
   "source": [
    "If we want to be really statistically robust we can actually run some different statistical tests like um ANOVA tests and effect size and Tuki tests to see um whether or not there is a meaningful statistical difference between the average temperature or the temperatures of these different classes um and we can see that indeed there is so we're doing some pairwise comparisons um so we can see for example that um on average um urban areas are more than seven degrees cooler than agricultural areas um and so uh they are definitely there's definitely a statistically significant difference there um.\n",
    "Which is interesting at the same time um urban areas are.\n",
    "Ending.\n",
    "Student building are hotter than um... um...\n",
    "Water body. water body. water body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Sample data first (much faster for all tests)\n",
    "sample_size = 50000\n",
    "df_sample = df_clean.sample(n=min(sample_size, len(df_clean)), random_state=42)\n",
    "\n",
    "# 1. One-way ANOVA: Test if ANY categories differ\n",
    "categories = df_sample[\"Category\"].unique()\n",
    "groups = [df_sample[df_sample[\"Category\"] == cat][\"LST\"].values for cat in categories]\n",
    "\n",
    "f_stat, p_value = stats.f_oneway(*groups)\n",
    "print(f\"ANOVA F-statistic: {f_stat:.2f}\")\n",
    "print(f\"p-value: {p_value:.2e}\")\n",
    "print(f\"Significant? {'YES' if p_value < 0.05 else 'NO'}\\n\")\n",
    "\n",
    "# 2. Effect size (eta-squared) on sample\n",
    "ss_between = sum([len(g) * (g.mean() - df_sample[\"LST\"].mean()) ** 2 for g in groups])\n",
    "ss_total = sum([(x - df_sample[\"LST\"].mean()) ** 2 for x in df_sample[\"LST\"]])\n",
    "eta_squared = ss_between / ss_total\n",
    "print(f\"Eta-squared (effect size): {eta_squared:.3f}\")\n",
    "print(\"  (0.01=small, 0.06=medium, 0.14=large)\\n\")\n",
    "\n",
    "# 3. Tukey HSD: Which pairs differ?\n",
    "print(\"Running Tukey HSD (this may take a moment)...\")\n",
    "tukey = pairwise_tukeyhsd(\n",
    "    endog=df_sample[\"LST\"], groups=df_sample[\"Category\"], alpha=0.05\n",
    ")\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6408a7",
   "metadata": {},
   "source": [
    "Now, going to the next step, we can start to just look at the distribution. We want to narrow our analysis to just urban areas. So we can do that by just filtering for the land surface temperature in urban areas and starting to look at the distribution of it. The mean and the median. We can see that it's actually fairly normally distributed with a long tail to the right. That means that most of our values are sort of clustered around that mean and median of 35.2, 46.62, but that there are some really extreme values for the upper end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb19dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually, instead of filtering by pixel, filter for within the urban area\n",
    "# there's rural, periurban, and urban\n",
    "# we'll want to compare distribution across areas\n",
    "# look at the distribution of temperatures across each\n",
    "# focus on just the urban areas?\n",
    "\n",
    "\n",
    "# also, for modeling:\n",
    "# add distance to green space\n",
    "# distance to industrial areas\n",
    "# look at the layer Parques Industriales y Anexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ecaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the homework assignment, *just* look at the core urban area of pergamino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96271dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for just \"Área sin vegetación\" (ID 22)\n",
    "df_bare = df_clean[df_clean[\"Category\"] == \"Área sin vegetación\"]\n",
    "\n",
    "print(f\"Píxeles de 'Área sin vegetación': {len(df_bare):,}\")\n",
    "print(f\"Rango LST: {df_bare['LST'].min():.2f}°C - {df_bare['LST'].max():.2f}°C\")\n",
    "\n",
    "# Histogram\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(\n",
    "    df_bare[\"LST\"],\n",
    "    bins=50,\n",
    "    color=\"#d4271e\",  # Use the MapBiomas color for this class\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax.axvline(\n",
    "    df_bare[\"LST\"].mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Media: {df_bare['LST'].mean():.2f}°C\",\n",
    ")\n",
    "ax.axvline(\n",
    "    df_bare[\"LST\"].median(),\n",
    "    color=\"darkred\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mediana: {df_bare['LST'].median():.2f}°C\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"LST Máxima (°C)\", fontsize=12)\n",
    "ax.set_ylabel(\"Frecuencia (píxeles)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Distribución de Temperatura Superficial Máxima\\nÁrea sin Vegetación, Pergamino (2020-2025)\",\n",
    "    fontsize=14,\n",
    "    pad=15,\n",
    ")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05244f0",
   "metadata": {},
   "source": [
    "We can get into breaking this into quantiles. So next, we'll actually break this into our quartiles. So, you know, 25%\n",
    "25 to 50 to 50, 50 to 75, 50 to 75, and top 25%, 25 to 50 to 75 and top 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all quantiles at once\n",
    "quantiles = df_bare[\"LST\"].quantile([0.25, 0.50, 0.75, 1.0, 0.90, 0.95])\n",
    "\n",
    "print(\"Quantiles de LST - Área sin Vegetación:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Q1 (25%):   {quantiles[0.25]:.2f}°C\")\n",
    "print(f\"Q2 (50%):   {quantiles[0.50]:.2f}°C\")\n",
    "print(f\"Q3 (75%):   {quantiles[0.75]:.2f}°C\")\n",
    "print(f\"Q4 (100%):  {quantiles[1.0]:.2f}°C\")\n",
    "print(f\"Top 10%:    {quantiles[0.90]:.2f}°C\")\n",
    "print(f\"Top 5%:     {quantiles[0.95]:.2f}°C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6cd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quantiles\n",
    "quantiles = df_bare[\"LST\"].quantile([0.75, 0.90, 0.95])\n",
    "\n",
    "# Create mask\n",
    "bare_mask = suelo_aligned == 22\n",
    "\n",
    "# Top 25% (Q75+)\n",
    "lst_top25 = LST_max_computed.where(\n",
    "    bare_mask & (LST_max_computed >= quantiles[0.75])\n",
    ").squeeze()\n",
    "lst_top25.rio.to_raster(lst_tif_path.parent / \"LST_top25_bare.tif\")\n",
    "\n",
    "# Top 10% (Q90+)\n",
    "lst_top10 = LST_max_computed.where(\n",
    "    bare_mask & (LST_max_computed >= quantiles[0.90])\n",
    ").squeeze()\n",
    "lst_top10.rio.to_raster(lst_tif_path.parent / \"LST_top10_bare.tif\")\n",
    "\n",
    "# Top 5% (Q95+)\n",
    "lst_top5 = LST_max_computed.where(\n",
    "    bare_mask & (LST_max_computed >= quantiles[0.95])\n",
    ").squeeze()\n",
    "lst_top5.rio.to_raster(lst_tif_path.parent / \"LST_top5_bare.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbeb4b",
   "metadata": {},
   "source": [
    "## Edificios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b37214",
   "metadata": {},
   "source": [
    "Next, we're going to pull in a global building data set that includes estimates of global building height. Now, there's likely a significant margin of error in these estimates, but we're going to aggregate them in a way where we think that aggregating them will kind of reduce the impact. We're going to take the total built volume per 30 meter pixel cell. We're going to do that as kind of a proxy for shadow. Basically, the idea that the more built area, the larger the buildings are, the more shadow there's going to be, and therefore, the more cooling effect. This is not necessarily true, right? It's likely to be more complicated than this. But, you know, depending on the material of the buildings and how absorbent of heat they are and whether they're really tall or really wide, but as a rough proxy, the cooling effect of shadow will use this. And we're going to go on to do some statistical evaluations to see how legit this feature is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be076c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "WEB_MERCATOR_CRS = \"EPSG:3857\"\n",
    "\n",
    "# Your existing AOI geometry\n",
    "# pergamino = gpd.read_file(\"pergamino.geojson\")  # Example if not already loaded\n",
    "CRS_WGS84 = \"EPSG:4326\"\n",
    "\n",
    "# Output file\n",
    "EDIFICACIONES_RUTA = \"edificaciones_pergamino_gba.parquet\"\n",
    "\n",
    "# 1️⃣ Compute bounding box in WGS84\n",
    "aoi_bounds = pergamino.to_crs(CRS_WGS84).total_bounds  # [xmin, ymin, xmax, ymax]\n",
    "xmin, ymin, xmax, ymax = aoi_bounds\n",
    "print(f\"Caja delimitadora de Pergamino: {aoi_bounds}\")\n",
    "\n",
    "\n",
    "# 2️⃣ Compute overlapping 5°×5° tiles\n",
    "def tile_name(lon_min, lat_min):\n",
    "    \"\"\"Return a tile filename component given SW corner (lon_min, lat_min).\"\"\"\n",
    "    ew = \"e\" if lon_min >= 0 else \"w\"\n",
    "    ns = \"n\" if lat_min >= 0 else \"s\"\n",
    "    lon_tag = f\"{abs(int(lon_min)):03d}\"\n",
    "    lat_tag = f\"{abs(int(lat_min)):02d}\"\n",
    "    return ew + lon_tag + \"_\" + ns + lat_tag\n",
    "\n",
    "\n",
    "# Compute all overlapping 5° grid indices\n",
    "lon_starts = range(\n",
    "    int(math.floor(xmin / 5.0)) * 5, int(math.ceil(xmax / 5.0)) * 5 + 5, 5\n",
    ")\n",
    "lat_starts = range(\n",
    "    int(math.floor(ymin / 5.0)) * 5, int(math.ceil(ymax / 5.0)) * 5 + 5, 5\n",
    ")\n",
    "\n",
    "tiles = []\n",
    "for lon0 in lon_starts:\n",
    "    for lat0 in lat_starts:\n",
    "        lon1 = lon0 + 5\n",
    "        lat1 = lat0 + 5\n",
    "        name = f\"{tile_name(lon0, lat1)}_{tile_name(lon1, lat0)}.parquet\"\n",
    "        tiles.append(name)\n",
    "\n",
    "print(\"Posibles tiles a usar:\")\n",
    "for t in tiles:\n",
    "    print(\"  \", t)\n",
    "\n",
    "# 3️⃣ Base URL for dataset\n",
    "BASE_URL = \"https://data.source.coop/tge-labs/globalbuildingatlas-lod1/\"\n",
    "urls = [BASE_URL + t for t in tiles]\n",
    "\n",
    "# 4️⃣ Connect to DuckDB\n",
    "con = duckdb.connect()\n",
    "for cmd in [\n",
    "    \"INSTALL spatial\",\n",
    "    \"LOAD spatial\",\n",
    "    \"INSTALL httpfs\",\n",
    "    \"LOAD httpfs\",\n",
    "    \"SET s3_url_style='path'\",\n",
    "    \"SET s3_use_ssl=true\",\n",
    "]:\n",
    "    con.execute(cmd)\n",
    "\n",
    "# 5️⃣ Query & filter each tile by AOI bbox\n",
    "if os.path.exists(EDIFICACIONES_RUTA):\n",
    "    print(\"Cargando edificaciones existentes...\")\n",
    "    edificaciones = gpd.read_parquet(EDIFICACIONES_RUTA)\n",
    "else:\n",
    "    print(\"Descargando y filtrando datos de Global Building Atlas (LOD1)...\")\n",
    "\n",
    "    aoi_filter = f\"\"\"\n",
    "    bbox.xmax >= {xmin} AND bbox.xmin <= {xmax} AND\n",
    "    bbox.ymax >= {ymin} AND bbox.ymin <= {ymax}\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine all matching tiles\n",
    "    query = f\"\"\"\n",
    "    COPY (\n",
    "        SELECT id, height, geometry\n",
    "        FROM read_parquet({urls}, filename=true)\n",
    "        WHERE {aoi_filter}\n",
    "    ) TO '{EDIFICACIONES_RUTA}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "\n",
    "    con.execute(query)\n",
    "    print(\"Datos guardados localmente como Parquet.\")\n",
    "\n",
    "    # Load into GeoDataFrame\n",
    "    df = gpd.read_parquet(EDIFICACIONES_RUTA)\n",
    "    # The geometry column is already shapely Polygon objects\n",
    "    edificaciones = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=CRS_WGS84)\n",
    "\n",
    "\n",
    "# 6️⃣ Clip to AOI\n",
    "edificaciones = edificaciones.to_crs(CRS_ARGENTINA)\n",
    "edificaciones = edificaciones[edificaciones.intersects(pergamino.geometry.iloc[0])]\n",
    "print(f\"Total de edificaciones dentro de Pergamino: {len(edificaciones)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c393d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edificaciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edificaciones[\"height\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edificaciones.hist(\"height\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edificaciones.plot(\"height\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c5251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geocube.api.core import make_geocube\n",
    "\n",
    "# 1. Calculate building volume (footprint area × height)\n",
    "edificaciones[\"area\"] = edificaciones.geometry.area  # in CRS units (m²)\n",
    "edificaciones[\"volume\"] = edificaciones[\"area\"] * edificaciones[\"height\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e721d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reproject template to match buildings CRS (meters)\n",
    "template_projected = template.rio.reproject(\"EPSG:5347\")\n",
    "\n",
    "# 2. Recalculate areas in proper CRS\n",
    "edificaciones_aligned = edificaciones.to_crs(\"EPSG:5347\")  # Should already be there\n",
    "edificaciones_aligned[\"area\"] = edificaciones_aligned.geometry.area\n",
    "edificaciones_aligned[\"volume\"] = edificaciones_aligned[\"area\"] * edificaciones_aligned[\n",
    "    \"height\"\n",
    "].fillna(0)\n",
    "\n",
    "# 3. Rasterize using projected template\n",
    "building_cube = make_geocube(\n",
    "    vector_data=edificaciones_aligned,\n",
    "    measurements=[\"volume\"],\n",
    "    like=template_projected,\n",
    "    fill=0,\n",
    ")\n",
    "\n",
    "# 4. Calculate pixel area in meters (now correct)\n",
    "pixel_width = abs(float(template_projected.rio.resolution()[0]))\n",
    "pixel_height = abs(float(template_projected.rio.resolution()[1]))\n",
    "pixel_area = pixel_width * pixel_height\n",
    "\n",
    "print(f\"Pixel size: {pixel_width:.2f} × {pixel_height:.2f} m\")\n",
    "print(f\"Pixel area: {pixel_area:.2f} m²\")\n",
    "\n",
    "# 5. Normalize properly\n",
    "volume_density = building_cube[\"volume\"] / pixel_area\n",
    "\n",
    "print(\n",
    "    f\"Volume density range: {float(volume_density.min()):.2f} - {float(volume_density.max()):.2f} m³/m²\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54652ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get non-zero values for histogram\n",
    "values = volume_density.values.flatten()\n",
    "values_nonzero = values[values > 0]\n",
    "\n",
    "print(f\"Total pixels: {len(values):,}\")\n",
    "print(f\"Non-zero pixels: {len(values_nonzero):,}\")\n",
    "print(f\"Range: {values_nonzero.min():.2f} - {values_nonzero.max():.2f} m³/m²\")\n",
    "\n",
    "# Histogram\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(values_nonzero, bins=50, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\n",
    "ax.axvline(\n",
    "    values_nonzero.mean(),\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Media: {values_nonzero.mean():.2f} m³/m²\",\n",
    ")\n",
    "ax.axvline(\n",
    "    np.median(values_nonzero),\n",
    "    color=\"darkred\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mediana: {np.median(values_nonzero):.2f} m³/m²\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Densidad Volumétrica (m³/m²)\", fontsize=12)\n",
    "ax.set_ylabel(\"Frecuencia (píxeles)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Distribución de Densidad Volumétrica de Edificaciones\\nPergamino (solo píxeles con edificaciones)\",\n",
    "    fontsize=14,\n",
    "    pad=15,\n",
    ")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional stats\n",
    "print(\"\\nEstadísticas:\")\n",
    "print(f\"  Media: {values_nonzero.mean():.2f} m³/m²\")\n",
    "print(f\"  Mediana: {np.median(values_nonzero):.2f} m³/m²\")\n",
    "print(f\"  Desv. Est.: {values_nonzero.std():.2f} m³/m²\")\n",
    "print(f\"  Percentil 95: {np.percentile(values_nonzero, 95):.2f} m³/m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984ffc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc30182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out 0 values for plotting\n",
    "volume_density_masked = volume_density.where(volume_density > 0)\n",
    "\n",
    "# Optional: Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "volume_density_masked.plot(\n",
    "    ax=ax,\n",
    "    cmap=\"viridis\",\n",
    "    cbar_kwargs={\"label\": \"Densidad volumétrica edificaciones (m³/m²)\"},\n",
    "    robust=True,\n",
    ")\n",
    "ax.set_title(\"Densidad Volumétrica de Edificaciones\\n(Área × Altura / Área de Píxel)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Align all rasters to same grid\n",
    "volume_density_aligned = volume_density.rio.reproject_match(LST_max_computed)\n",
    "suelo_aligned = suelo_2022.rio.reproject_match(LST_max_computed)\n",
    "\n",
    "# 2. Flatten arrays\n",
    "lst_flat = LST_max_computed.values.flatten()\n",
    "density_flat = volume_density_aligned.values.flatten()\n",
    "landcover_flat = suelo_aligned.values.flatten()\n",
    "\n",
    "# 3. Filter for urban pixels (ID 22) with valid data\n",
    "urban_mask = (\n",
    "    (landcover_flat == 22)\n",
    "    & (~np.isnan(lst_flat))\n",
    "    & (~np.isnan(density_flat))\n",
    "    & (density_flat > 0)\n",
    ")\n",
    "\n",
    "lst_urban = lst_flat[urban_mask]\n",
    "density_urban = density_flat[urban_mask]\n",
    "\n",
    "print(f\"Píxeles urbanos válidos: {len(lst_urban):,}\")\n",
    "\n",
    "# 4. Scatterplot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.hexbin(density_urban, lst_urban, gridsize=40, cmap=\"YlOrRd\", mincnt=1)\n",
    "plt.colorbar(scatter, ax=ax, label=\"Número de píxeles\")\n",
    "\n",
    "ax.set_xlabel(\"Densidad Volumétrica Edificaciones (m³/m²)\", fontsize=12)\n",
    "ax.set_ylabel(\"LST Máxima (°C)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Relación entre Densidad de Edificaciones y Temperatura Superficial\\nÁrea sin Vegetación, Pergamino (2020-2025)\",\n",
    "    fontsize=14,\n",
    "    pad=15,\n",
    ")\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation\n",
    "correlation = np.corrcoef(density_urban, lst_urban)[0, 1]\n",
    "print(f\"\\nCorrelación: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2028e",
   "metadata": {},
   "source": [
    "Now, we can actually compare the relationship between building density and land surface temperature in...\n",
    "In urban areas. And we'll see that if we do a little log transformation of the building density numbers to normalize the distribution a little bit more. There is a low but positive correlation. So as building density increases, land surface temperature increases too. which is interesting because we would have expected perhaps a shadow effect or something to play in here, but it's, you know, it's complicated. So basically, what we're seeing is that in denser locations, buildings with more built volume, there is a land surface temperature is warmer or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ad7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform density (add small constant to avoid log(0))\n",
    "density_urban_log = np.log10(density_urban + 0.01)\n",
    "\n",
    "# Scatterplot with log-transformed density\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.hexbin(density_urban_log, lst_urban, gridsize=40, cmap=\"YlOrRd\", mincnt=1)\n",
    "plt.colorbar(scatter, ax=ax, label=\"Número de píxeles\")\n",
    "\n",
    "# Add linear regression line\n",
    "from scipy import stats\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "    density_urban_log, lst_urban\n",
    ")\n",
    "\n",
    "# On your existing plot:\n",
    "x_line = np.linspace(density_urban_log.min(), density_urban_log.max(), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(\n",
    "    x_line,\n",
    "    y_line,\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    label=f\"y = {slope:.2f}x + {intercept:.2f}\\nr = {r_value:.3f}\",\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"log₁₀(Densidad Volumétrica) [m³/m²]\", fontsize=12)\n",
    "ax.set_ylabel(\"LST Máxima (°C)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Relación entre Densidad de Edificaciones (log) y Temperatura Superficial\\nÁrea sin Vegetación, Pergamino (2020-2025)\",\n",
    "    fontsize=14,\n",
    "    pad=15,\n",
    ")\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with log-transformed data\n",
    "correlation_log = np.corrcoef(density_urban_log, lst_urban)[0, 1]\n",
    "print(f\"\\nCorrelación (densidad log-transformada): {correlation_log:.3f}\")\n",
    "print(\n",
    "    f\"Correlación (densidad original): {np.corrcoef(density_urban, lst_urban)[0, 1]:.3f}\"\n",
    ")\n",
    "\n",
    "# Distribution check\n",
    "print(\"\\nDensidad original:\")\n",
    "print(\n",
    "    f\"  Percentiles: 25%={np.percentile(density_urban, 25):.2f}, 50%={np.percentile(density_urban, 50):.2f}, 75%={np.percentile(density_urban, 75):.2f}, 95%={np.percentile(density_urban, 95):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502dc8e4",
   "metadata": {},
   "source": [
    "## Cursos de agua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1e3eb",
   "metadata": {},
   "source": [
    "As another feature, we're going to load distance to water. So we're going to pull in the water bodies from the official data portal of the municipality of the Camino and we're going to... we are going to similarly calculate the relationship between...\n",
    "The distance of a given pixel to a body of water and the temperature of that pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "\n",
    "# Base WMS URL (clean up the malformed URL)\n",
    "wms_url = \"https://ide.pergamino.gob.ar:8443/geoserver/wms\"\n",
    "layer_name = \"publico:cursos_de_agua\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df68811",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfs_url = \"https://ide.pergamino.gob.ar:8443/geoserver/wfs\"\n",
    "layer_name = \"publico:cursos_de_agua\"\n",
    "\n",
    "# Build GetFeature request\n",
    "bbox = pergamino.to_crs(\"EPSG:4326\").total_bounds\n",
    "\n",
    "params = {\n",
    "    \"service\": \"WFS\",\n",
    "    \"version\": \"2.0.0\",\n",
    "    \"request\": \"GetFeature\",\n",
    "    \"typename\": layer_name,\n",
    "    \"outputFormat\": \"application/json\",\n",
    "    \"srsname\": \"EPSG:4326\",\n",
    "    \"bbox\": f\"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]},EPSG:4326\",\n",
    "}\n",
    "\n",
    "response = requests.get(wfs_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    cursos_agua_gdf = gpd.read_file(BytesIO(response.content))\n",
    "    print(f\"Loaded {len(cursos_agua_gdf)} watercourse features\")\n",
    "    print(f\"CRS: {cursos_agua_gdf.crs}\")\n",
    "\n",
    "    # Clip to exact Pergamino boundary\n",
    "    cursos_agua_gdf = cursos_agua_gdf.to_crs(pergamino.crs)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursos_agua_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2feb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursos_agua_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7844fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio import features\n",
    "import numpy as np\n",
    "\n",
    "# 1. Align watercourses to LST grid\n",
    "cursos_agua_aligned = cursos_agua_gdf.to_crs(LST_max_computed.rio.crs)\n",
    "\n",
    "# 2. Get transform and shape from LST raster\n",
    "transform = LST_max_computed.rio.transform()\n",
    "shape = LST_max_computed.shape\n",
    "\n",
    "# 3. Rasterize water features directly\n",
    "water_mask = features.rasterize(\n",
    "    shapes=cursos_agua_aligned.geometry,\n",
    "    out_shape=shape,\n",
    "    transform=transform,\n",
    "    fill=0,\n",
    "    default_value=1,\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "\n",
    "# 4. Calculate distance transform\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "\n",
    "distance_pixels = distance_transform_edt(~water_mask.astype(bool))\n",
    "\n",
    "# Convert to meters\n",
    "pixel_size = abs(float(LST_max_computed.rio.resolution()[0]))\n",
    "distance_meters = distance_pixels * pixel_size\n",
    "\n",
    "# 5. Create xarray with proper coords\n",
    "distance_to_water = LST_max_computed.copy()\n",
    "distance_to_water.values = distance_meters\n",
    "distance_to_water.name = \"distance_to_water\"\n",
    "\n",
    "print(\n",
    "    f\"Distance range: {distance_to_water.min().values:.0f} - {distance_to_water.max().values:.0f} meters\"\n",
    ")\n",
    "\n",
    "# 6. Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "distance_to_water.plot(\n",
    "    ax=ax1, cmap=\"Blues_r\", cbar_kwargs={\"label\": \"Distancia al agua (m)\"}\n",
    ")\n",
    "ax1.set_title(\"Distancia a Cursos de Agua\")\n",
    "\n",
    "LST_max_computed.plot(\n",
    "    ax=ax2, cmap=\"afmhot\", robust=True, cbar_kwargs={\"label\": \"LST (°C)\"}\n",
    ")\n",
    "ax2.set_title(\"LST Máxima\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec164ff0",
   "metadata": {},
   "source": [
    "Now again, running a basic correlation analysis, we will see...\n",
    "That, that, that, um,\n",
    "The correlation is once again Positive Small but it's positive Basically As we get farther away From the body of water The temperature On average goes up The temperature tends to increase Again this is not necessarily A causative relationship This is just correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Align all rasters to same grid\n",
    "distance_aligned = distance_to_water.values.flatten()\n",
    "lst_flat = LST_max_computed.values.flatten()\n",
    "landcover_flat = suelo_aligned.values.flatten()\n",
    "\n",
    "# 2. Filter for urban pixels (ID 22) with valid data\n",
    "urban_mask = (\n",
    "    (landcover_flat == 22) & (~np.isnan(lst_flat)) & (~np.isnan(distance_aligned))\n",
    ")\n",
    "\n",
    "lst_urban = lst_flat[urban_mask]\n",
    "distance_urban = distance_aligned[urban_mask]\n",
    "\n",
    "print(f\"Píxeles urbanos válidos: {len(lst_urban):,}\")\n",
    "\n",
    "# 3. Scatterplot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.hexbin(distance_urban, lst_urban, gridsize=40, cmap=\"YlOrRd\", mincnt=1)\n",
    "plt.colorbar(scatter, ax=ax, label=\"Número de píxeles\")\n",
    "\n",
    "# 4. Add linear regression line\n",
    "from scipy import stats\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "    distance_urban, lst_urban\n",
    ")\n",
    "\n",
    "x_line = np.linspace(distance_urban.min(), distance_urban.max(), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(\n",
    "    x_line,\n",
    "    y_line,\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    label=f\"y = {slope:.4f}x + {intercept:.2f}\\nr = {r_value:.3f}\",\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Distancia a Cursos de Agua (m)\", fontsize=12)\n",
    "ax.set_ylabel(\"LST Máxima (°C)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Relación entre Distancia al Agua y Temperatura Superficial\\nÁrea sin Vegetación, Pergamino (2020-2025)\",\n",
    "    fontsize=14,\n",
    "    pad=15,\n",
    ")\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Stats\n",
    "correlation = np.corrcoef(distance_urban, lst_urban)[0, 1]\n",
    "print(f\"\\nCorrelación: {correlation:.3f}\")\n",
    "print(\"Distancia al agua:\")\n",
    "print(\n",
    "    f\"  Percentiles: 25%={np.percentile(distance_urban, 25):.0f}m, 50%={np.percentile(distance_urban, 50):.0f}m, 75%={np.percentile(distance_urban, 75):.0f}m, 95%={np.percentile(distance_urban, 95):.0f}m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd0054",
   "metadata": {},
   "source": [
    "## Arbolado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15f2af",
   "metadata": {},
   "source": [
    "There that we're going to pull in Is tree canopy So again we're going to pull in Tree canopy data derived from Satellite imagery From By municipality Pergamino If we didn't want to use this We could also use World Resources Institute One meter resolution Tree canopy data set So there is an open source Global alternative to doing this But in this case Since we have local data We're going to use the local data And all we're going to do Is calculate the percent Tree canopy cover Per pixel cell You know obviously It's not getting into questions Of canopy height And canopy health And all that stuff But as a basic proxy For tree cover This will serve our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geopandas as gpd\n",
    "\n",
    "arbolado_layer_name = \"publico:cobertura_arbolado_urbano\"\n",
    "\n",
    "params = {\n",
    "    \"service\": \"WFS\",\n",
    "    \"version\": \"2.0.0\",\n",
    "    \"request\": \"GetFeature\",\n",
    "    \"typename\": arbolado_layer_name,\n",
    "    \"outputFormat\": \"application/json\",\n",
    "    \"srsname\": \"EPSG:4326\",\n",
    "    \"bbox\": f\"{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]},EPSG:4326\",\n",
    "}\n",
    "\n",
    "response = requests.get(wfs_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    arbolado_gdf = gpd.read_file(BytesIO(response.content))\n",
    "    print(f\"Loaded {len(arbolado_gdf)} tree features\")\n",
    "\n",
    "    arbolado_gdf = arbolado_gdf.to_crs(pergamino.crs)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ddad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbolado_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbolado_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f553e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbolado_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio import features\n",
    "import numpy as np\n",
    "\n",
    "# 1. Align canopy to LST grid\n",
    "cobertura_aligned = arbolado_gdf.to_crs(LST_max_computed.rio.crs)\n",
    "\n",
    "# 2. Get grid info from LST\n",
    "transform = LST_max_computed.rio.transform()\n",
    "shape = LST_max_computed.shape\n",
    "pixel_size = abs(float(LST_max_computed.rio.resolution()[0]))\n",
    "pixel_area = pixel_size**2  # 30m × 30m = 900 m²\n",
    "\n",
    "print(f\"Grid: {shape}, Pixel size: {pixel_size}m, Pixel area: {pixel_area} m²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0124c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create empty array for canopy coverage\n",
    "canopy_coverage = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "# 4. Rasterize: sum of intersecting canopy areas per pixel\n",
    "print(\"Rasterizing tree canopy...\")\n",
    "\n",
    "# Use a finer internal resolution to better capture partial coverage\n",
    "# Rasterize at higher resolution, then aggregate\n",
    "upscale = 10  # 10x finer = 3m resolution internally\n",
    "fine_shape = (shape[0] * upscale, shape[1] * upscale)\n",
    "fine_transform = transform * transform.scale(1 / upscale, 1 / upscale)\n",
    "\n",
    "# Rasterize canopy as binary mask at fine resolution\n",
    "fine_mask = features.rasterize(\n",
    "    shapes=cobertura_aligned.geometry,\n",
    "    out_shape=fine_shape,\n",
    "    transform=fine_transform,\n",
    "    fill=0,\n",
    "    default_value=1,\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "\n",
    "# 5. Aggregate to target resolution (percentage coverage)\n",
    "# Reshape and calculate mean (= % coverage at finer resolution)\n",
    "canopy_percent = (\n",
    "    fine_mask.reshape(shape[0], upscale, shape[1], upscale).mean(axis=(1, 3)) * 100\n",
    ")  # Convert to percentage\n",
    "\n",
    "# 6. Create xarray aligned with LST\n",
    "canopy_coverage_xr = LST_max_computed.copy()\n",
    "canopy_coverage_xr.values = canopy_percent\n",
    "canopy_coverage_xr.name = \"canopy_coverage_percent\"\n",
    "\n",
    "print(\n",
    "    f\"Canopy coverage range: {canopy_coverage_xr.min().values:.2f}% - {canopy_coverage_xr.max().values:.2f}%\"\n",
    ")\n",
    "print(f\"Mean coverage: {canopy_coverage_xr.mean().values:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "canopy_coverage_xr.plot(\n",
    "    ax=ax1,\n",
    "    cmap=\"Greens\",\n",
    "    vmin=0,\n",
    "    vmax=100,\n",
    "    cbar_kwargs={\"label\": \"Cobertura arbórea (%)\"},\n",
    ")\n",
    "ax1.set_title(\"Cobertura Arbórea (%)\")\n",
    "\n",
    "LST_max_computed.plot(\n",
    "    ax=ax2, cmap=\"afmhot\", robust=True, cbar_kwargs={\"label\": \"LST (°C)\"}\n",
    ")\n",
    "ax2.set_title(\"LST Máxima\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63fc01",
   "metadata": {},
   "source": [
    "Now in contrast To what we've seen before When we calculate Correlation here We see that there's actually A negative correlation That's stronger than The correlations we've seen before So basically as tree canopy cover Increases The The.\n",
    "Land surface temperature Goes down Right So there's an inverse Relationship here That more tree canopy cover Means a lower Land surface temperature Which is what we expect To see And this is So far the strongest Correlation of any of the Features that we've looked at Suggest that Land surface temperature Is most closely Related to tree canopy In comparison to Building density And Distance to water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Flatten all rasters\n",
    "canopy_flat = canopy_coverage_xr.values.flatten()\n",
    "lst_flat = LST_max_computed.values.flatten()\n",
    "landcover_flat = suelo_aligned.values.flatten()\n",
    "\n",
    "# 2. Filter for urban pixels (ID 22) with valid data\n",
    "urban_mask = (landcover_flat == 22) & (~np.isnan(lst_flat)) & (~np.isnan(canopy_flat))\n",
    "\n",
    "lst_urban = lst_flat[urban_mask]\n",
    "canopy_urban = canopy_flat[urban_mask]\n",
    "\n",
    "print(f\"Píxeles urbanos válidos: {len(lst_urban):,}\")\n",
    "\n",
    "# 3. Scatterplot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "scatter = ax.hexbin(canopy_urban, lst_urban, gridsize=40, cmap=\"YlOrRd\", mincnt=1)\n",
    "plt.colorbar(scatter, ax=ax, label=\"Número de píxeles\")\n",
    "\n",
    "# 4. Add linear regression line\n",
    "from scipy import stats\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(canopy_urban, lst_urban)\n",
    "\n",
    "x_line = np.linspace(canopy_urban.min(), canopy_urban.max(), 100)\n",
    "y_line = slope * x_line + intercept\n",
    "ax.plot(\n",
    "    x_line,\n",
    "    y_line,\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    label=f\"y = {slope:.4f}x + {intercept:.2f}\\nr = {r_value:.3f}\",\n",
    ")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Cobertura Arbórea (%)\", fontsize=12)\n",
    "ax.set_ylabel(\"LST Máxima (°C)\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Relación entre Cobertura Arbórea y Temperatura Superficial\\nÁrea sin Vegetación, Pergamino (2020-2025)\",\n",
    "    fontsize=14,\n",
    "    pad=15,\n",
    ")\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Stats\n",
    "correlation = np.corrcoef(canopy_urban, lst_urban)[0, 1]\n",
    "print(f\"\\nCorrelación: {correlation:.3f}\")\n",
    "print(\"Cobertura arbórea:\")\n",
    "print(\n",
    "    f\"  Percentiles: 25%={np.percentile(canopy_urban, 25):.1f}%, 50%={np.percentile(canopy_urban, 50):.1f}%, 75%={np.percentile(canopy_urban, 75):.1f}%, 95%={np.percentile(canopy_urban, 95):.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2fd1bf",
   "metadata": {},
   "source": [
    "## Modelación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ca0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare data (same as before)\n",
    "canopy_flat = canopy_coverage_xr.values.flatten()\n",
    "density_flat = volume_density_aligned.values.flatten()\n",
    "distance_flat = distance_to_water.values.flatten()\n",
    "lst_flat = LST_max_computed.values.flatten()\n",
    "landcover_flat = suelo_aligned.values.flatten()\n",
    "\n",
    "urban_mask = (\n",
    "    (landcover_flat == 22)\n",
    "    & (~np.isnan(lst_flat))\n",
    "    & (~np.isnan(canopy_flat))\n",
    "    & (~np.isnan(density_flat))\n",
    "    & (~np.isnan(distance_flat))\n",
    ")\n",
    "\n",
    "df_urban = pd.DataFrame(\n",
    "    {\n",
    "        \"LST\": lst_flat[urban_mask],\n",
    "        \"Canopy_Coverage\": canopy_flat[urban_mask],\n",
    "        \"Building_Density\": density_flat[urban_mask],\n",
    "        \"Distance_Water\": distance_flat[urban_mask],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Urban pixels for modeling: {len(df_urban):,}\\n\")\n",
    "\n",
    "# 2. Individual models\n",
    "print(\"=\" * 70)\n",
    "print(\"INDIVIDUAL MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "predictors = [\n",
    "    (\"Canopy Coverage\", df_urban[\"Canopy_Coverage\"]),\n",
    "    (\"Building Density (log)\", np.log10(df_urban[\"Building_Density\"] + 0.01)),\n",
    "    (\"Distance to Water\", df_urban[\"Distance_Water\"]),\n",
    "]\n",
    "\n",
    "for name, X in predictors:\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(X, df_urban[\"LST\"])\n",
    "    r2 = r_value**2\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  R² = {r2:.4f} ({r2 * 100:.2f}% variance explained)\")\n",
    "    print(f\"  Coefficient = {slope:.4f}, p-value = {p_value:.2e}\")\n",
    "\n",
    "# 3. Multiple Linear Regression using numpy\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTIPLE LINEAR REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Design matrix (add intercept column)\n",
    "X = np.column_stack(\n",
    "    [\n",
    "        np.ones(len(df_urban)),  # Intercept\n",
    "        df_urban[\"Canopy_Coverage\"],\n",
    "        np.log10(df_urban[\"Building_Density\"] + 0.01),\n",
    "        df_urban[\"Distance_Water\"],\n",
    "    ]\n",
    ")\n",
    "y = df_urban[\"LST\"].values\n",
    "\n",
    "# Ordinary Least Squares: β = (X'X)^(-1) X'y\n",
    "beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Predictions\n",
    "y_pred = X @ beta\n",
    "\n",
    "# R²\n",
    "ss_res = np.sum((y - y_pred) ** 2)\n",
    "ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "r2_multi = 1 - (ss_res / ss_tot)\n",
    "adj_r2 = 1 - (1 - r2_multi) * (len(y) - 1) / (len(y) - X.shape[1])\n",
    "\n",
    "print(f\"\\nR² = {r2_multi:.4f} ({r2_multi * 100:.2f}% variance explained)\")\n",
    "print(f\"Adjusted R² = {adj_r2:.4f}\")\n",
    "\n",
    "print(\"\\nCoefficients:\")\n",
    "feature_names = [\n",
    "    \"Intercept\",\n",
    "    \"Canopy Coverage (%)\",\n",
    "    \"Building Density (log10)\",\n",
    "    \"Distance to Water (m)\",\n",
    "]\n",
    "for name, coef in zip(feature_names, beta):\n",
    "    print(f\"  {name:30s}: {coef:+.4f}\")\n",
    "\n",
    "# 4. Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0].hexbin(y, y_pred, gridsize=50, cmap=\"viridis\", mincnt=1)\n",
    "axes[0].plot(\n",
    "    [y.min(), y.max()], [y.min(), y.max()], \"r--\", lw=2, label=\"Perfect prediction\"\n",
    ")\n",
    "axes[0].set_xlabel(\"LST Observada (°C)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"LST Predicha (°C)\", fontsize=12)\n",
    "axes[0].set_title(f\"Predicción del Modelo (R² = {r2_multi:.3f})\", fontsize=13)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y - y_pred\n",
    "axes[1].hexbin(y_pred, residuals, gridsize=50, cmap=\"RdYlBu_r\", mincnt=1)\n",
    "axes[1].axhline(0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "axes[1].set_xlabel(\"LST Predicha (°C)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Residuos (°C)\", fontsize=12)\n",
    "axes[1].set_title(\"Residuos del Modelo\", fontsize=13)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCombined model explains {r2_multi * 100:.1f}% of LST variance in urban areas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f086a7",
   "metadata": {},
   "source": [
    "Now here we're going to use Some modeling Some statistical modeling Not to try to predict anything But to explain Understand The relationship Between land surface temperature And these various Factors Because Now here we're going to use Some modeling Now here we're going to use.\n",
    "Now something that's really Important to note here Is that We're dealing with Spatial data Spatial data When we model it Always has What we call Spatial process Or almost always Which means that Closer things are more related Than farther away things So we have to factor In the relationship Not just to the thing For the pixel itself But in the area around the pixel This is called Spatial process So in this case We're going to model that By looking not only At the impact of Tree canopy cover Or distance to water Or building density But also of Those The average value Of all those features In the area Surrounding a given pixel So the neighborhood There are different ways To handle this But we're going to calculate A neighborhood Of 450 meters Which is What came out As most significant When I did some Small Linear regression models Here And tried different values So we're going to throw those features in and we're going to see that indeed.\n",
    "They have an explanatory force here beyond just the immediate pixel so basically it's not just important what tree canopy cover is in a pixel itself but also how much tree canopy cover there is in the neighborhood surrounding that pixel it's really important to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0020cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as sp_stats\n",
    "\n",
    "# Rerun optimal model with significance tests\n",
    "sigma = 7.0\n",
    "print(f\"Final Model (sigma={sigma}, ~420m radius)\\n\")\n",
    "\n",
    "# Smooth predictors\n",
    "canopy_smoothed = gaussian_filter(canopy_coverage_xr.values, sigma=sigma)\n",
    "density_smoothed = gaussian_filter(\n",
    "    np.log10(volume_density_aligned.values + 0.01), sigma=sigma\n",
    ")\n",
    "distance_smoothed = gaussian_filter(distance_to_water.values, sigma=sigma)\n",
    "\n",
    "df_urban[\"Canopy_Lag\"] = canopy_smoothed.flatten()[urban_mask]\n",
    "df_urban[\"Density_Lag\"] = density_smoothed.flatten()[urban_mask]\n",
    "df_urban[\"Distance_Lag\"] = distance_smoothed.flatten()[urban_mask]\n",
    "\n",
    "# Build model\n",
    "X_spatial = np.column_stack(\n",
    "    [\n",
    "        np.ones(len(df_urban)),\n",
    "        df_urban[\"Canopy_Coverage\"],\n",
    "        np.log10(df_urban[\"Building_Density\"] + 0.01),\n",
    "        df_urban[\"Distance_Water\"],\n",
    "        df_urban[\"Canopy_Lag\"],\n",
    "        df_urban[\"Density_Lag\"],\n",
    "        df_urban[\"Distance_Lag\"],\n",
    "    ]\n",
    ")\n",
    "y = df_urban[\"LST\"].values\n",
    "\n",
    "# Fit model\n",
    "beta = np.linalg.lstsq(X_spatial, y, rcond=None)[0]\n",
    "y_pred = X_spatial @ beta\n",
    "\n",
    "# Calculate standard errors and p-values\n",
    "n = len(y)\n",
    "k = X_spatial.shape[1] - 1  # Number of predictors (excluding intercept)\n",
    "residuals = y - y_pred\n",
    "mse = np.sum(residuals**2) / (n - k - 1)\n",
    "\n",
    "# Variance-covariance matrix\n",
    "var_covar = mse * np.linalg.inv(X_spatial.T @ X_spatial)\n",
    "std_errors = np.sqrt(np.diag(var_covar))\n",
    "\n",
    "# t-statistics and p-values\n",
    "t_stats = beta / std_errors\n",
    "p_values = 2 * (1 - sp_stats.t.cdf(np.abs(t_stats), df=n - k - 1))\n",
    "\n",
    "# 95% confidence intervals\n",
    "ci_95 = 1.96 * std_errors\n",
    "\n",
    "# Print results\n",
    "feature_names = [\n",
    "    \"Intercept\",\n",
    "    \"Canopy (%)\",\n",
    "    \"Building (log)\",\n",
    "    \"Distance (m)\",\n",
    "    \"Neighbor Canopy (%)\",\n",
    "    \"Neighbor Building (log)\",\n",
    "    \"Neighbor Distance (m)\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    f\"{'Variable':<28s} | {'Coef':>8s} | {'Std Err':>8s} | {'t-stat':>8s} | {'p-value':>10s} | {'Sig':>4s}\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (name, coef, se, t, p) in enumerate(\n",
    "    zip(feature_names, beta, std_errors, t_stats, p_values)\n",
    "):\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "    print(f\"{name:<28s} | {coef:+8.4f} | {se:8.4f} | {t:+8.2f} | {p:10.2e} | {sig:>4s}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Significance: *** p<0.001, ** p<0.01, * p<0.05\\n\")\n",
    "\n",
    "# Practical interpretations\n",
    "print(\"=\" * 80)\n",
    "print(\"PRACTICAL EFFECTS (holding other variables constant)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nLocal Effects (your pixel):\")\n",
    "print(f\"  +10% canopy          → {beta[1] * 10:+.2f}°C\")\n",
    "print(f\"  10× building density → {beta[2]:+.2f}°C (log scale)\")\n",
    "print(f\"  +1000m from water    → {beta[3] * 1000:+.2f}°C\")\n",
    "\n",
    "print(\"\\nNeighborhood Effects (within 420m):\")\n",
    "print(f\"  +10% canopy          → {beta[4] * 10:+.2f}°C  ← STRONGEST COOLING\")\n",
    "print(f\"  10× building density → {beta[5]:+.2f}°C  ← HUGE COOLING (shadows!)\")\n",
    "print(f\"  +1000m from water    → {beta[6] * 1000:+.2f}°C\")\n",
    "\n",
    "print(\"\\nComposite Scenario - Adding Trees to Neighborhood:\")\n",
    "print(\"  Increase neighborhood canopy from 10% → 30%\")\n",
    "print(f\"  Effect: {beta[4] * 20:+.2f}°C cooling\")\n",
    "print(\"  (= planting street trees across 4-5 blocks)\")\n",
    "\n",
    "# R² info\n",
    "r2 = 1 - (np.sum(residuals**2) / np.sum((y - y.mean()) ** 2))\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"  R² = {r2:.4f} ({r2 * 100:.1f}% variance explained)\")\n",
    "print(f\"  RMSE = {np.sqrt(mse):.2f}°C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ca407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Test range of sigma values\n",
    "sigma_values = [0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "results = []\n",
    "\n",
    "print(\"Testing different sigma values...\\n\")\n",
    "print(f\"{'Sigma':>6s} | {'Radius (m)':>10s} | {'R²':>6s} | {'Adj R²':>7s}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for sigma in sigma_values:\n",
    "    # Smooth each predictor\n",
    "    canopy_smoothed = gaussian_filter(canopy_coverage_xr.values, sigma=sigma)\n",
    "    density_smoothed = gaussian_filter(\n",
    "        np.log10(volume_density_aligned.values + 0.01), sigma=sigma\n",
    "    )\n",
    "    distance_smoothed = gaussian_filter(distance_to_water.values, sigma=sigma)\n",
    "\n",
    "    # Extract for urban pixels\n",
    "    canopy_lag = canopy_smoothed.flatten()[urban_mask]\n",
    "    density_lag = density_smoothed.flatten()[urban_mask]\n",
    "    distance_lag = distance_smoothed.flatten()[urban_mask]\n",
    "\n",
    "    # Build model\n",
    "    X_spatial = np.column_stack(\n",
    "        [\n",
    "            np.ones(len(df_urban)),\n",
    "            df_urban[\"Canopy_Coverage\"],\n",
    "            np.log10(df_urban[\"Building_Density\"] + 0.01),\n",
    "            df_urban[\"Distance_Water\"],\n",
    "            canopy_lag,\n",
    "            density_lag,\n",
    "            distance_lag,\n",
    "        ]\n",
    "    )\n",
    "    y = df_urban[\"LST\"].values\n",
    "\n",
    "    beta = np.linalg.lstsq(X_spatial, y, rcond=None)[0]\n",
    "    y_pred = X_spatial @ beta\n",
    "\n",
    "    ss_res = np.sum((y - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    adj_r2 = 1 - (1 - r2) * (len(y) - 1) / (len(y) - X_spatial.shape[1])\n",
    "\n",
    "    radius_m = sigma * 2 * 30  # Approximate 2-sigma radius in meters (95% of weight)\n",
    "    results.append({\"sigma\": sigma, \"radius_m\": radius_m, \"r2\": r2, \"adj_r2\": adj_r2})\n",
    "\n",
    "    print(f\"{sigma:6.1f} | {radius_m:10.0f} | {r2:6.4f} | {adj_r2:7.4f}\")\n",
    "\n",
    "# Find optimal\n",
    "results_df = pd.DataFrame(results)\n",
    "best = results_df.loc[results_df[\"r2\"].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 45)\n",
    "print(f\"OPTIMAL: sigma = {best['sigma']:.1f} (≈{best['radius_m']:.0f}m radius)\")\n",
    "print(f\"R² = {best['r2']:.4f} ({best['r2'] * 100:.2f}%)\")\n",
    "print(f\"Adjusted R² = {best['adj_r2']:.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(\n",
    "    results_df[\"sigma\"], results_df[\"r2\"], \"o-\", linewidth=2, markersize=8, label=\"R²\"\n",
    ")\n",
    "ax.plot(\n",
    "    results_df[\"sigma\"],\n",
    "    results_df[\"adj_r2\"],\n",
    "    \"s--\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    label=\"Adjusted R²\",\n",
    ")\n",
    "ax.axvline(\n",
    "    best[\"sigma\"],\n",
    "    color=\"red\",\n",
    "    linestyle=\":\",\n",
    "    alpha=0.7,\n",
    "    label=f\"Optimal σ={best['sigma']:.1f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Sigma (pixels)\", fontsize=12)\n",
    "ax.set_ylabel(\"R²\", fontsize=12)\n",
    "ax.set_title(\"Model Performance vs Spatial Neighborhood Size\", fontsize=13)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef735740",
   "metadata": {},
   "source": [
    "Finally we can use a random forest model.\n",
    "And shamely additive and shamely additive.\n",
    "I don't remember what it is, SHAPs, whatever it's called, SHAP values to actually assess the relative importance of variables now we're going to need to do some spatial cross-validation here for the spatial process, read more about that here and what that means and why that matters basically we can account for how much of the variance of data across pixels this model explains and how much we think the impact would be of say a 10% increase in tree canopy cover."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
