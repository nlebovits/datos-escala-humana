# Open Data for Cities and Climate

## Objectives

Understand what open data is, where it comes from, how to find it, and how to evaluate its quality. Understand the main sources of open data for climate and urban risk at local, national, and global scales.

## What is Open Data

Open data is, in effect, data that anyone can use. This is an approximate definition, subject to interpretation and nuance depending on context, but generally that is the idea.

Beyond technical availability, open data typically carries legal permissions that allow use, redistribution, and derivative works. Common open licenses include Creative Commons (CC-BY, CC-BY-SA) and Open Database License (ODbL). The specific license determines what you can do with the data—whether you can use it commercially, whether you must attribute the source, whether derivative works must use the same license. When evaluating a dataset for municipal use, check the license terms to ensure they match your intended use case.

### Advantages

Open data offers fundamental properties that make it particularly valuable for climate risk work in resource-constrained contexts:

**Interoperability**: Open formats and standards allow data from different sources to work together. You can combine Copernicus elevation data with OpenStreetMap infrastructure and local census data without proprietary conversion tools.

**Auditability**: Open data allows you to trace methods, verify quality, and understand limitations. When a flood model uses a particular DEM, you can examine that DEM's resolution, accuracy, and coverage rather than accepting a black box.

**No vendor lock-in**: Open data doesn't tie you to a specific software vendor or platform. If your GIS software changes or a commercial platform shuts down, your data and workflows remain accessible.

**Reproducibility**: Open data enables others to verify your analysis and adapt your methods to their context. This is essential for building institutional knowledge and capacity across cities facing similar challenges.

For resource-constrained municipalities, these properties matter because they enable access to high-quality data without the budget for commercial alternatives, allow comparison across regions using consistent methodologies, and reduce dependence on external technical capacity that may not be sustainable long-term.

### Growth in Volume

The volume of open data has increased rapidly. The open data era begins in the 2010s, but particularly in recent years it has taken off with the rise of cloud storage, AI models, and new massive data volumes.

## Geospatial Data Infrastructure

Open data is found in geospatial data infrastructure: any place where open geospatial data is found on the internet. It can be a catalog, server, web application, list of pages, or layers.

Geospatial data infrastructure includes both technical components (APIs, tile servers, STAC catalogs) and institutional arrangements (who maintains it, how it's funded, governance structures). Understanding both dimensions matters when evaluating whether to rely on a data source for municipal planning. A technically excellent catalog with uncertain funding may disappear when a grant expires. An institutionally stable source with poor documentation may be difficult to use effectively.

The main objective is to ensure collaboration and accessibility of geospatial information. Generally, you will look for an open geospatial portal at national and regional scale.

When evaluating a geospatial data portal, consider: Is it maintained by a stable institution (national mapping agency, established research organization) or dependent on project funding? Does it have clear documentation and support channels? Has it been operational for multiple years? Is there a community of users you can learn from? These questions help assess whether a data source is reliable enough to base planning decisions on.

### Recent Important Changes

Volume and quality have grown tremendously. Many of these datasets are available globally. A large part of this course focuses on globally available datasets to fill local or national gaps.

## Main Sources of Open Data

Important repositories to explore:

**OpenStreetMap**: Community-generated geographic data. Coverage and quality vary significantly by region—strong in areas with active mapping communities, sparse elsewhere. Best for infrastructure (roads, buildings) in urban areas. Data quality depends on local contributor activity. Institutionally stable (OpenStreetMap Foundation), but data completeness is community-dependent.

**Google Earth Engine**: Platform providing access to satellite imagery and geospatial datasets, particularly strong for environmental monitoring and land cover analysis. Requires learning the Earth Engine API. Free for research and non-commercial use. Backed by Google's infrastructure, making it highly stable, but tied to Google's platform.

**Google Earth Engine Community Catalog**: Community-curated collection of datasets not in the official Earth Engine catalog. More diverse than the official catalog, but sustainability depends on individual contributors and grant funding. Variable documentation quality. Excellent for finding specialized climate datasets.

**OpenLandMap**: Provides global environmental layers (soil, climate, vegetation) based on machine learning models. Focus on reproducible science and open methods. Strong documentation of methods and uncertainty. Institutionally backed by OpenGeoHub Foundation. Particularly useful for environmental baseline data.

**Source Cooperative**: Community-driven data catalog emphasizing cloud-native formats and open science. Newer platform (launched 2021) with growing collection of climate and earth observation data. Sustainability model still developing. Good for finding GeoParquet and cloud-optimized datasets.

**Microsoft Planetary Computer**: Curated catalog of earth observation data optimized for analysis at scale. Strong focus on cloud-native formats and computational efficiency. Backed by Microsoft, making it institutionally stable. US-government-focused datasets (USGS, NOAA), which means better coverage of North America than other regions.

Many have QGIS plugins to import data directly. If not, you can always access in Python or through download. This book primarily uses Python workflows to demonstrate reproducible, cloud-native approaches that work across these platforms.

### Limitations of Global Data

**Not so global**: Global datasets often come from richer countries in the global north, China, and Japan, not from global south countries.

**Lack of precision**: Data, especially derived from machine learning models, is only as good as it is comprehensive. Systematic bias toward certain countries and away from others means that models typically are less accurate or datasets less comprehensive in countries with less data.

Models often perform better outside the global south because they have less data from the global south.

**Evaluating fitness for your region**: When encountering a "global" dataset, assess its usefulness for your specific location:

- **Visual inspection**: Load the data for your area and compare it against your local knowledge. Does the land cover classification match what you know about your city? Do building footprints align with actual development patterns?

- **Ground truthing**: If possible, verify the data against field observations or local surveys. Check a few locations you know well—does the dataset accurately represent them?

- **Comparison with local data**: If you have local government data (even if less current), compare it with the global dataset. Where do they agree? Where do they diverge? Understanding these differences helps you know what the global data can and cannot tell you.

- **Check the documentation**: Read the dataset's methodology and validation reports. Does it include accuracy assessments for your region? What were the training data sources? If validation focuses on North America and Europe, the dataset likely performs worse in your context.

- **Understand resolution and scale**: A 30m resolution global land cover dataset may be adequate for regional analysis but inappropriate for neighborhood-scale planning. Match the data resolution to your decision scale.

Global datasets are tools, not truth. They provide starting points and regional context, but they require critical evaluation before use in local decision-making. When global data quality is insufficient for your needs, consider hybrid approaches that combine global baselines with local data collection, or focus efforts on collecting the specific high-priority data your decisions require.

## Geospatial File Formats

This section describes different formats for storing and accessing geospatial data. In Chapter 1, we demonstrated several of these approaches: loading vector data from a WFS service (ARBA municipalities), accessing raster data via STAC catalogs (Copernicus DEM), and querying cloud-optimized files over HTTP (MapBiomas land cover). Understanding these format categories helps you navigate the data landscape and choose appropriate access methods for different situations.

### Flat or Standard Files

**Examples**: GeoJSON, shapefile.

**Uses**: Local work or web visualization. Can be considered legacy files. Excellent for working offline.

**Limitations**: If you want to work at large scale with massive global datasets, GeoJSON or shapefile is not the way to do it. They are also more difficult to share.

GeoJSON is specifically designed for web visualization, but has significant limitations.

### WMS and WFS Servers

Standardized protocols for web mapping and data visualization. Integration with GIS, but can be slow for queries. Can be considered precursors to cloud-native formats because they were the industry standard for a long time for sharing online. They remain useful, but are aging.

In Chapter 1, we used a WFS service to load Buenos Aires Province municipalities from ARBA's GeoServer. WFS remains common for government data portals, particularly in Latin America where many agencies have invested in this infrastructure.

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt
import os
from owslib.wfs import WebFeatureService
from io import StringIO, BytesIO

partidos_path = "../../public/python-geoespacial/pba_partidos.parquet"

# Load partidos from ARBA GeoServer WFS service
if os.path.exists(partidos_path):
    partidos = gpd.read_parquet(partidos_path)
else:
    # Connect to WFS (Web Feature Service)
    wfs_url = "https://geo.arba.gov.ar/geoserver/idera/wfs"
    wfs = WebFeatureService(url=wfs_url, version="2.0.0")

    # Request the Departamento layer (partidos)
    response = wfs.getfeature(
        typename="idera:Departamento",
        srsname="EPSG:5347",  # Original CRS
    )

    # Convert WFS response to GeoDataFrame
    partidos = gpd.read_file(BytesIO(response.read()))

    # Save to cache for future use
    partidos.to_parquet(partidos_path)

# Reproject to working CRS
partidos = partidos.to_crs("EPSG:5348")  # POSGAR 2007 / Argentina 4

la_plata = partidos[partidos["fna"] == "Partido de La Plata"]

la_plata.head()
```

### APIs

Flexible ways to get exactly the data you want, making data available through an endpoint. Excellent for programming workflows and web applications, but require technical knowledge. Not always as efficient as optimized cloud-native formats.

API and cloud-native geospatial data can be compatible, as below, when we load Copernicus DEM data via the Element84 STAC API:

```{python}
import pystac_client
from odc.stac import load
import rioxarray as rio

# Reproject to geographic CRS for bounding box query
aoi = la_plata.to_crs("EPSG:4326")

# Configure AWS for unsigned requests (no login required)
os.environ["AWS_NO_SIGN_REQUEST"] = "YES"
os.environ["GDAL_DISABLE_READDIR_ON_OPEN"] = "EMPTY_DIR"

# Connect to Element84 Earth Search STAC catalog
catalog = pystac_client.Client.open("https://earth-search.aws.element84.com/v1")

# Get bounding box for our area of interest
# bbox format: [min_x, min_y, max_x, max_y]
bbox = aoi.total_bounds

# Search for Copernicus DEM tiles covering our area
# cop-dem-glo-30 is the global 30m elevation dataset
search = catalog.search(
    collections=["cop-dem-glo-30"],
    bbox=bbox,
)

# Get the items (files) that cover our area
items = list(search.items())

# Load DSM using chunks for efficient memory processing
# odc.stac.load automatically downloads and assembles the necessary tiles
dsm = load(
    items,
    bbox=bbox,
    chunks={"x": 1024, "y": 1024},  # Process in blocks for efficiency
)

# Extract elevation band and convert to DataArray for use with rioxarray
# load() returns a Dataset; we need the elevation DataArray
dsm = dsm["data"].squeeze()  # Remove extra dimensions

# Clip raster to exact geometries
dsm_clipped = dsm.rio.clip(aoi.geometry)

# Reproject clipped area to working CRS (POSGAR 2007 / Argentina 4)
# This is more efficient because we reproject only the area of interest
dsm_reproj = dsm_clipped.rio.reproject(dst_crs="EPSG:5348")
```

#### Cloud-Native Geospatial

**Cloud-native geospatial** tools are designed specifically to work with data stored in the cloud. Unlike traditional tools that require downloading complete datasets, these tools allow you to access and process only the portions of data needed through protocols like HTTP range requests.

::: {.column-margin}
The **cloud** basically means someone else’s computer: servers that your device connects to over the internet. Cloud computing offers power, flexibility, and scalability as needed without requiring you to configure local servers. It provides distributed and reliable infrastructure. However, it has an initial learning curve, costs can accumulate with intensive use, and it requires constant internet connection.
:::

Examples include Cloud Optimized GeoTIFF (COG) for raster data, which allows efficient streaming of satellite images without downloading complete files. GeoParquet and FlatGeobuf are cloud-optimized vector formats that support fast spatial queries. Platforms like Google Earth Engine, Microsoft Planetary Computer, and AWS Earth Search provide massive catalogs of satellite data with APIs for analysis at scale.

These tools are especially relevant for climate risk analysis, where we need to process large volumes of satellite, climate, and geographic data without the infrastructure to store them locally. The cloud-native tools we will use are not more difficult than traditional tools and offer many advantages. The cloud is simply one of the places where people compute today. As mentioned in [the introduction](../intro.qmd#guiding-principles), this book takes the cloud-native approach as the default.

Instead of importing all the global data (several terabytes), we import only our area of interest through **lazy loading**—the library doesn't transfer data until you request specific operations, letting you work with massive datasets while downloading only the small portion you need. We'll cover cloud storage, APIs, and remote data access in detail in [chapter two](/en/open-data/open-data.qmd).

### Cloud-Optimized Formats

Best for sharing online and for massive live data. They are a step forward over traditional file formats, although they have their own limitations.

**Main advantage**: They allow you to query only the data you want for your area of interest. Instead of having to download gigabytes of files, you might only download megabytes and you can download them very efficiently only for the area that matters to you. For our purposes, this is the main advantage.

Cloud-optimized formats like GeoParquet (for vector data) and Cloud Optimized GeoTIFF/COG (for raster data) support HTTP range requests, meaning you can read just the portion of a file you need without downloading the whole thing. This is transformative for working with large datasets—instead of downloading Argentina's entire building footprint dataset, you can query just the buildings in La Plata. In Chapter 1, we demonstrated this by loading elevation data from a global DEM and land cover data from a national dataset, downloading only our area of interest.

STAC (SpatioTemporal Asset Catalog) provides standardized metadata and discovery for cloud-optimized data. When we queried the Copernicus DEM in Chapter 1, we used a STAC catalog to find the specific tiles covering La Plata, then loaded only those tiles. This pattern—discover via STAC, load via cloud-optimized formats—is increasingly standard for earth observation and climate data.

```{python}
suelo_2022_ruta = "https://storage.googleapis.com/mapbiomas-public/initiatives/argentina/collection-1/coverage/argentina_coverage_2022.tif"

# Open the raster with rioxarray and chunks
suelo_2022 = rio.open_rasterio(
    suelo_2022_ruta,
    chunks={"x": 4096, "y": 4096},
)

# Clip in the raster's native CRS (EPSG:4326)
suelo_2022_clipped = suelo_2022.rio.clip(
    aoi.geometry.values,
    from_disk=True,
)

# Reproject to working CRS (POSGAR 2007 / Argentina 4)
suelo_2022 = suelo_2022_clipped.rio.reproject(dst_crs="EPSG:5348")

# Mask NoData values
suelo_2022_masked = suelo_2022.where(
    (suelo_2022 != suelo_2022.rio.nodata) & (suelo_2022 != 255)
)
```

```{python}
import duckdb
import pandas as pd

# Get bounding box of La Plata in WGS84 (EPSG:4326) for cloud query
bounds_4326 = la_plata.to_crs("EPSG:4326").total_bounds

# Configure DuckDB with spatial and network extensions
con = duckdb.connect()
con.execute("INSTALL spatial")
con.execute("LOAD spatial")
con.execute("INSTALL httpfs")
con.execute("LOAD httpfs")

# Configure DuckDB for S3 cloud storage access
con.execute("SET s3_region='us-east-1'")
con.execute("SET s3_endpoint='data.source.coop'")
con.execute("SET s3_use_ssl=true")
con.execute("SET s3_url_style='path'")

# Query buildings within our bounding box and save to temporary file
# This filters buildings using geographic bounds (bbox) to load only those in our area of interest
temp_file = "buildings_filtered.parquet"
query = f"""
COPY (
    SELECT *
    FROM 's3://vida/google-microsoft-open-buildings/geoparquet/by_country/country_iso=ARG/ARG.parquet'
    WHERE bbox.xmax >= {bounds_4326[0]} AND bbox.xmin <= {bounds_4326[2]} AND
          bbox.ymax >= {bounds_4326[1]} AND bbox.ymin <= {bounds_4326[3]}
) TO '{temp_file}' (FORMAT PARQUET);
"""
con.execute(query)

# Read filtered building data from Parquet file
buildings_df = pd.read_parquet(temp_file)

# Deserialize geometries: convert from binary format (WKB - Well-Known Binary) to GeoSeries objects
# GeoParquet stores geometries as bytes in WKB format, we need to convert them to GeoPandas geometries
buildings_df["geometry"] = gpd.GeoSeries.from_wkb(buildings_df["geometry"])

# Create GeoDataFrame with deserialized geometries
# Specify original CRS as EPSG:4326 (WGS84) since data comes in that coordinate system
buildings = gpd.GeoDataFrame(buildings_df, geometry="geometry", crs="EPSG:4326")

# Reproject to working CRS to match other analysis data
buildings = buildings.to_crs("EPSG:5348")
```

### When to Use Which Format

For cloud-native workflows, GeoParquet is a columnar format optimized for analytical queries of large vector datasets, and FlatGeobuf provides efficient spatial access through built-in spatial indexing. Both support HTTP range requests, meaning you can query subsets of data without downloading entire files.

You'll also encounter GeoJSON (.geojson, .json), which is used for web mapping and stores coordinates as text using JavaScript Object Notation. GeoPackage (.gpkg) is a single-file format that works well for local data exchange. ESRI Shapefile (.shp, .dbf, .shx) is a legacy format that requires multiple files and has technical limitations (field name length, attribute types, file size), but you'll still encounter it in the wild because many organizations haven't migrated yet.

**Use cloud-native formats (GeoParquet, COG, Zarr) when:**

- Working with large datasets where you only need a subset
- Accessing data stored remotely (cloud storage, data repositories)
- Building reproducible workflows that others can run without downloading large files
- Processing earth observation or climate data at scale

**Use WFS/WMS services when:**

- Accessing government data portals that provide these endpoints
- You need the most current version of regularly updated data
- Integration with existing GIS workflows

**Use flat files (GeoJSON, GeoPackage) when:**

- Working with small datasets that fit comfortably in memory
- You need offline access to data
- Sharing data with users who lack technical infrastructure for cloud-native access

This book emphasizes cloud-native approaches by default because they enable the scale and efficiency needed for climate risk analysis in resource-constrained contexts. The workflows demonstrated here assume internet access and basic computational resources (laptop, moderate bandwidth) but don't require expensive commercial software or high-end infrastructure.
