## Geospatial Python

_This lesson is largely based on material from [Introduction to Geospatial Raster and Vector Data with Python](https://carpentries-incubator.github.io/geospatial-python/index.html) by The Carpentries, available under a [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license._

### Objectives

After completing this chapter, you will be able to load and work with vector and raster data in Python; reproject data between different coordinate reference systems; and perform key spatial operations such as clipping, aggregation, and zonal statistics.

### What is GIS?

**GIS (Geographic Information System)** is software that allows you to interact with geospatial data: capture, store, analyze, and visualize information about locations on the Earth's surface. These tools are fundamental for understanding spatial patterns, making informed decisions about resources and territory, and communicating geographic information effectively.

In the context of climate risk management, GIS makes it possible to do things such as identify areas vulnerable to flooding, analyze urban temperature patterns, evaluate green infrastructure coverage, and much more. Common tools include desktop applications (ArcGIS, QGIS), spatial databases (PostGIS), processing libraries (GDAL), cloud platforms (Google Earth Engine), and programming languages (R and Python).

There are two main approaches for working with geospatial data:

- **Graphical user interface (GUI)** applications like ArcGIS and QGIS allow you to work with spatial data without learning a programming language. Mapping and visualization are more intuitive and flexible in this environment. However, these tools have important limitations: low reproducibility of analyses, limited scalability for automation, restricted capacity to customize functionality, and complex interfaces with too many buttons that can be overwhelming.
- On the other hand, **programming** offers high reproducibility, scalability, and complete automation. It provides total flexibility to customize functionality and facilitates integration with other workflows and data sources. The main disadvantages are the need to learn a programming language, a steeper learning curve at the beginning, and that mapping and visualization are less intuitive initially.

In this book, we use **Python** for all of our analyses. Python is widely used with a rich ecosystem of specialized libraries for geospatial analysis. It is flexible, powerful, and relatively easy to learn compared to other languages. It offers complete integration with databases, APIs, web services, and other programming languages. Additionally, it is free, open source, and has an active community that provides abundant documentation, tutorials, and support. Lastly, its high replicability makes it ideal for sharing analyses that can be adapted and scaled in other contexts.

### Coordinate Reference Systems (CRS)

Before we get into looking at any data, we need to define a key concept in geospatial data analysis: **coordinate reference systems (CRS)**. [Coordinate reference systems](https://py.geocompx.org/01-spatial-data#sec-coordinate-reference-systems-intro) define how the spatial elements of data relate to the Earth's surface. CRS are geographic or projected. [Geographic systems](https://py.geocompx.org/01-spatial-data#geographic-coordinate-systems) identify locations using longitude and latitude in decimal degrees on a spherical or ellipsoidal surface. [Projected systems](https://py.geocompx.org/01-spatial-data#sec-projected-coordinate-reference-systems) convert the three-dimensional surface of the Earth into Cartesian coordinates (x, y) in meters on an implicitly flat surface. Any time you work with spatial data, it should have a CRS (if it doesn't, that's almost definitely an error), so it's important to have a good grasp of CRS from the outset.

![Examples of different CRS, Opennews.org via [@carpentries_geospatial_python]](../../public/python-geoespacial/us-crs-versions.jpg)

All projection introduces deformations. Therefore, some properties of the Earth's surface are distorted: area, direction, distance, and shape. A projection can preserve only one or two of these properties. Projections are named according to the property they preserve: equal-area preserves area, azimuthal preserves direction, equidistant preserves distance, and conformal preserves local shape.

If you imagine that the Earth is an orange, the way you peel it and then flatten the skin is similar to how projections are made.

![Orange metaphor [@carpentries_geospatial_python]](../../public/python-geoespacial/orange-peel-earth.jpg)

::: {.callout-tip}
When problems arise with spatial analysis, the cause is often CRS-related issues. When troubleshooting, one of the first things you should check is whether there is a CRS problem.
:::

## Vector data

The [vector data model](https://py.geocompx.org/01-spatial-data#sec-vector-data) represents geographic features with points, lines, and polygons. These geometries have discrete and well-defined boundaries, which means that vector data generally has high precision. Points can represent independent features (like the location of a bus stop) or they can connect to form more complex geometries like lines and polygons.

![Vector data [@carpentries_geospatial_python]](../../public/python-geoespacial/spatial_extent.png)

### Importing vector data

Let's load in an example of vector data and explore it. The first thing we'll do is load the Python libraries that we need.

::: {.callout-tip}
[GeoPandas](https://geopandas.org/) is the core Python library for handling geospatial vector data. We also use [Matplotlib](https://matplotlib.org/) for basic graphics and visualization.
:::

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt
```

We load municipal boundaries for Buenos Aires Province from ARBA[@arba_partidos_pba]. The code below loads a local copy of the data stored as a GeoParquet file. In [chapter two](/en/open-data/open-data.qmd), we'll talk more about file formats and show how to query these same data from ARBA's WFS service. For now, it's enough to know that GeoParquet is one of several ways to store vector data.

:::{.callout-important}
In this chapter, we're loading data from our own local copies of remote files (stored on servers elsewhere). While this book assumes a cloud-native approach, we try to introduce new concepts "just-in-time" and therefore will show how to access the remote versions of these datasets in [chapter two](/en/open-data/open-data.qmd), when we discuss open data sources and access. Similarly, since this chapter focuses on geospatial processing, we'll save our in depth explanation of why we choose _these_ particular datasets for chapters [two](/en/open-data/open-data.qmd) and [three](/en/climate-risk/climate-risk.qmd).
:::

```{python}
partidos_path = "../../public/python-geoespacial/pba_partidos.parquet"

# Load partidos from our local copy of the data from the ARBA GeoServer WFS service
partidos = gpd.read_parquet(partidos_path)
```

Now that we've loaded these data, we see what they look like. This is a book about geospatial data, so let's start by mapping them!

```{python}
partidos.plot()
```

### Reprojection

What we're looking at here is a map of all the administrative districts ("partidos" in Spanish) in the province of Buenos Aires. Before we do anything else, we'll make sure we've set the CRS properly. Transforming data from one coordinate system to another is frequently necessary when working with multiple data sources. When performing spatial analysis, all datasets must be in the same CRS. (This holds true for both vector and raster data, as [we will see below](#zonal-statistics)) We're going to be working with the Partido de Berisso, so we're going to use `EPSG:5348`, also know as Posgar 2007 / Argentina 4.

```{python}
# Reproject to our local CRS
partidos = partidos.to_crs("EPSG:5348")  # POSGAR 2007 / Argentina 4
```

::: {.callout-tip}
When you have datasets in different coordinate systems, choose a projected CRS that covers your study area. For Argentina, POSGAR 2007 divides the country into 7 Gauss-Krueger zones. Use the zone that covers your municipality. If your study area spans multiple zones, choose the zone that covers the majority of your area. You can confirm CRS codes at [epsg.io](https://epsg.io).
:::

### Data structure and basic filtering

Vector data is organized into a table of attributes and geometries. It builds on the concept of a dataframe, which is a tabular data structure with rows and columns. Each row represents an observation (e.g., a building, a river segment, a city), and each column represents an attribute of that observation (e.g., name, population, land cover type). The [geometry column](https://py.geocompx.org/01-spatial-data#sec-geometry-columns) is what makes our vector data a GeoDataFrame: it contains the geometric part of the vector layer and is the basis for all spatial operations. This column can contain point, line, polygon, or multipolygon, and also stores spatial reference information (CRS).

![GeoDataframe [@carpentries_geospatial_python]](../../public/python-geoespacial/pandas_geopandas_relation.png)

Let's take a peek at what's in our `partidos` data:

```{python}
partidos.head()
```

The `partidos` data has several attributes: department code, department name, and geometry. To extract only the data for the Partido de Berisso, we filter for the observation where the "fna" column corresponds to "Partido de Berisso". (If you're wondering whether we can also accomplish this by filtering based on a certain geographic area, good intuition! We come that in the [key spatial operations](#key-spatial-operations) below.)

```{python}
berisso = partidos[partidos["fna"] == "Partido de Berisso"]

berisso.head()
```

We can also plot the data to see what Berisso looks like on a map:

```{python}
berisso.plot()
```

We now have a basic understand of what vector data is and how to work with in Python. You'll see these concepts come up repeatedly throughout the book, so you'll have plenty more time to absorb them, but if you're interested in going deeper on your own, we recommend chapters one through three of [Geocomputation with Python](https://py.geocompx.org/). We'll move on to raster data next.

## Raster data

The [raster data model](https://py.geocompx.org/01-spatial-data#sec-raster-data) represents the world through a continuous grid of cells of constant size. This model consists of metadata (header) that defines the coordinate system, the origin, and the resolution, plus a matrix of values. The origin is typically the coordinate of the lower-left corner. This representation avoids explicitly storing the coordinates of each cell, making raster processing much more efficient than vector processing.

![Raster data [@carpentries_geospatial_python]](../../public/python-geoespacial/raster_concept.png)

Cell values are numeric, representing continuous variables (elevation, temperature, precipitation) or categorical variables (land cover types, use classes). Rasters usually represent continuous phenomena, although discrete features like soil classes can also be represented in this model. However, the edges of discrete features can become blurry in raster datasets.

::: {.column-margin}
**Continuous data** refer to values that can take any number within a range (including decimals). These values are stored as floats (floating-point numbers) instead of integers, allowing you to represent precise measurements like 15.7 meters or 23.42 meters of elevation. **Categorical data** represents categories by encoding them as integers.
:::

Rasters dominate many environmental sciences due to dependence on satellite remote sensing data. Examples include satellite images, aerial photographs, digital elevation models, and precipitation maps. In this chapter we use digital elevation models (continuous) and land cover data (categorical).

::: {.callout-tip}
[Rasterio](https://rasterio.readthedocs.io/) reads and writes georeferenced raster formats. [Xarray](https://xarray.dev/) and [rioxarray](https://corteva.github.io/rioxarray/) facilitate working with labeled multidimensional arrays, ideal for climate time series.
:::

### Raster file formats

GeoTIFF (.tif, .tiff) is the industry standard format for GIS applications and satellite remote sensing. Cloud Optimized GeoTIFF (COG) is an optimized variant that incorporates tiles and previews to support HTTP queries, allowing you to load subsets of the image without transferring the complete file. For cloud-native workflows with multidimensional data, Zarr is a format optimized for cloud storage, ideal for climate time series.

We work with 30m resolution Digital Surface Model data from the Copernicus DEM [@copernicus_dem_openlandmap], accessed via the Element84 Earth Search STAC catalog. We use Cloud Optimized GeoTIFF along with the `rioxarray` module to import only the necessary data efficiently. Instead of importing all the global data (several terabytes), we import only our area of interest through **lazy loading**—the library doesn't transfer data until you request specific operations, letting you work with massive datasets while downloading only the small portion you need. We'll cover cloud storage, APIs, and remote data access in detail in [chapter two](/en/open-data/open-data.qmd).

```{python}
import rioxarray as rio

# import a local copy of the Copernicus DSM
dsm = rio.open_rasterio("../../public/python-geoespacial/copernicus_dsm.tif")

# Reproject to working CRS (POSGAR 2007 / Argentina 4)
# Both vector and raster data can be reprojected
dsm = dsm.rio.reproject(dst_crs="EPSG:5348")
```

We visualize the digital surface model for Berisso. The `robust=True` parameter improves contrast by ignoring extreme values, and `cmap="terrain"` uses an appropriate color palette for elevation.

```{python}
#| code-fold: true
#| code-summary: "Show visualization code"

dsm.plot(robust=True, cmap="terrain")
```

## Key spatial operations

Now that we've seen how to load vector and raster data, we'll delve into some of the basic analyses that can be done with these data, focusing on a handful of those that often show up in climate risk assessments.

### Spatial clipping

When we pulled in the DEM data, we used the line `dsm_clipped = dsm.rio.clip(aoi.geometry)` to "clip" the raster to the boundaries of our area of interest (i.e., the boundaries of Berisso). **Spatial clipping** refers to extracting data (raster or vector) within a specific area of interest. Let's take a look at another example.

We import 30m resolution categorical land cover data from MapBiomas Argentina Collection 1 [@mapbiomas_argentina_2022], covering 1998-2022. This is the most reliable land cover data currently available for Argentina. The dataset is accessed directly from MapBiomas' public Google Cloud Storage bucket. We'll explain how to find and access datasets like this in [chapter two](/en/open-data/open-data.qmd). As we see here, the full dataset is a single file covering all of Argentina. That's a lot of data!

```{python}
#| code-fold: true
#| code-summary: "Show land cover colormap setup"

from matplotlib.colors import ListedColormap, BoundaryNorm
import numpy as np

# MapBiomas Argentina land cover categories
categories = {
    "Closed woody": {"ids": [3], "color": "#1f8d49"},
    "Open woody": {"ids": [4], "color": "#7dc975"},
    "Sparse woody": {"ids": [45], "color": "#807a40"},
    "Flooded woody": {"ids": [6], "color": "#026975"},
    "Flooded non-woody vegetation": {"ids": [11], "color": "#519799"},
    "Grassland": {"ids": [12], "color": "#d6bc74"},
    "Steppe": {"ids": [63], "color": "#ebf8b5"},
    "Pasture": {"ids": [15], "color": "#edde8e"},
    "Agriculture": {"ids": [18], "color": "#e974ed"},
    "Forest plantation": {"ids": [9], "color": "#7a5900"},
    "Shrub crop": {"ids": [36], "color": "#d082de"},
    "Agricultural mosaic": {"ids": [21], "color": "#ffefc3"},
    "Non-vegetated area": {"ids": [22], "color": "#d4271e"},
    "River, lake or ocean": {"ids": [33], "color": "#2532e4"},
    "Ice and surface snow": {"ids": [34], "color": "#93dfe6"},
    "Not observed": {"ids": [27], "color": "#ffffff"},
}

# Build colormap from categories (sorted by ID for consistent mapping)
colors_by_id = {id_: info["color"] for info in categories.values() for id_ in info["ids"]}
sorted_ids = sorted(colors_by_id.keys())
lc_cmap = ListedColormap([colors_by_id[i] for i in sorted_ids])
lc_norm = BoundaryNorm(sorted_ids + [max(sorted_ids) + 1], lc_cmap.N)
```

```{python}
suelo_2022_ruta = "https://storage.googleapis.com/mapbiomas-public/initiatives/argentina/collection-1/coverage/argentina_coverage_2022.tif"

# Load overview and mask nodata
overview = rio.open_rasterio(suelo_2022_ruta, overview_level=4).squeeze()
overview = overview.where((overview != 0) & (~np.isnan(overview)))
```

```{python}
#| code-fold: true
#| code-summary: "Show visualization code"

overview.plot(cmap=lc_cmap, norm=lc_norm, add_colorbar=False)
```

However, we're only interested in the data for the Berisso area, so we're going to clip the Mapbiomas raster to our area of interest. Because `rioxarray` is cloud-optimized, all this is done lazily, which means we're only actually loading in the bytes corresponding to our area of interest.

Now, we're left with just the data corresponding to the area we actually care about.

```{python}
# Reproject to geographic CRS for bounding box query
aoi = berisso.to_crs("EPSG:4326")

# Open the raster with rioxarray and chunks
suelo_2022 = rio.open_rasterio(
    suelo_2022_ruta,
    chunks={"x": 4096, "y": 4096},
)

# Clip in the raster's native CRS (EPSG:4326)
suelo_2022_clipped = suelo_2022.rio.clip(
    aoi.geometry.values,
    from_disk=True,
)

# Reproject to working CRS (POSGAR 2007 / Argentina 4)
suelo_2022 = suelo_2022_clipped.rio.reproject(dst_crs="EPSG:5348")

# Mask NoData values
suelo_2022_masked = suelo_2022.where(
    (suelo_2022 != suelo_2022.rio.nodata) & (suelo_2022 != 255)
)
```

```{python}
#| code-fold: true
#| code-summary: "Show plotting and legend code"

# Filter legend to only categories present in clipped data
present_ids = set(np.unique(suelo_2022_masked.values[~np.isnan(suelo_2022_masked.values)]).astype(int))
present_categories = {
    name: info for name, info in categories.items()
    if any(id_ in present_ids for id_ in info["ids"])
}

fig, ax = plt.subplots()
suelo_2022_masked.plot(ax=ax, cmap=lc_cmap, norm=lc_norm, add_colorbar=False)

legend = [plt.Rectangle((0, 0), 1, 1, facecolor=info["color"], label=name)
          for name, info in present_categories.items()]
ax.legend(handles=legend, loc="lower center", bbox_to_anchor=(0.5, -0.1), ncol=3, frameon=False)
plt.title("Land Cover - MapBiomas 2022")
plt.tight_layout()
plt.show()
```

### Spatial aggregation

What if we want to go beyond clipping to actually assessing how many observations in one dataset fall within the boundaries of another? We can do this using **spatial aggregation**, which allows us to assess how many observations of one vector dataset fall within (or outside of, or a number of [other spatial predicates](https://py.geocompx.org/03-spatial-operations#sec-vector-spatial-aggregation)) the geographic area of another vector dataset. (Spatial predicates are geometric relationships between objects: intersection, containment, proximity, etc. These operations are fundamental for analyses like identifying neighborhoods within flood zones or finding critical infrastructure near risk areas.) Here, we'll demonstrate this by counting building footprints from the Google-Microsoft-OSM Open Buildings dataset [@google_microsoft_osm_buildings] within Berisso's boundaries. Building counts proxy for exposure in risk assessments—more structures means more potential flood damage.

```{python}
# read local copy of the open building footprints dataset
buildings = gpd.read_parquet("../../public/python-geoespacial/vida-buildings.parquet")

# Reproject to working CRS to match other analysis data
buildings = buildings.to_crs("EPSG:5348")
```

Now we spatially filter buildings that fall within Berisso's boundaries and count them:

```{python}
# Spatial join: keep only buildings that intersect with Berisso polygon
buildings_in_berisso = gpd.sjoin(buildings, berisso, predicate="intersects", how="inner")

# Count total buildings
total_buildings = len(buildings_in_berisso)
```

```{python}
#| code-fold: true
#| code-summary: "Show map visualization code"

# Visualize buildings within Berisso
fig, ax = plt.subplots()

# Plot Berisso boundary
berisso.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=2)

# Plot buildings with orange outlines
buildings_in_berisso.plot(ax=ax, facecolor="none", edgecolor="orange", linewidth=0.5)

ax.set_title(f"Building Footprints in Berisso\nTotal buildings: {total_buildings:,}",
             fontsize=14, fontweight="bold")
ax.set_xticks([])
ax.set_yticks([])
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)
ax.spines["bottom"].set_visible(False)
ax.spines["left"].set_visible(False)

plt.tight_layout()
plt.show()
```

Beyond counting buildings, we often need to measure them. Building footprint area is a key metric for calculating exposure in climate risk assessments. GeoPandas calculates area directly from polygon geometries:

```{python}
# Calculate area in square meters (our CRS uses meters)
buildings_in_berisso["area_m2"] = buildings_in_berisso.geometry.area

# Calculate total built area
total_built_area_m2 = buildings_in_berisso["area_m2"].sum()
total_built_area_km2 = total_built_area_m2 / 1_000_000

print(f"Total building footprint area: {total_built_area_km2:.2f} km²")
print(f"Average building footprint: {buildings_in_berisso['area_m2'].mean():.1f} m²")
```

Note that area calculations depend on your CRS. If we had calculated areas in EPSG:4326 (geographic coordinates), the results would be wrong because degrees are not uniform units of distance. This is why we reprojected to EPSG:5348, an equal-area projection appropriate for this region.

### Zonal statistics

We've just looked at aggregating and summarizing one vector dataset by another. What if we want to aggregate or describe a _raster_ dataset? For example, taking the average land surface temperature within a municipal boundary. This kind of spatial operation is referred to as **zonal statistics**, which allow you to calculate aggregate values from a raster within zones defined by vector polygons. This operation is fundamental for connecting continuous data (raster) with administrative units (vector). In this example, we extract elevation statistics for the Berisso area, calculating the minimum, mean, median, and maximum values. Given Berisso's tendency to flood, understanding the spatial distribution of elevation is essential to identifying flood-prone areas.

```{python}
import numpy as np
from rasterstats import zonal_stats

berisso = berisso.copy()

# Extract elevation statistics (min, mean, median, max)
stats = zonal_stats(
    berisso,
    dsm.squeeze().values,
    affine=dsm.rio.transform(),
    stats=["min", "mean", "median", "max"],
    nodata=np.nan,
)

# Add the statistics as new columns
berisso["dsm_min"] = [s["min"] for s in stats]
berisso["dsm_mean"] = [s["mean"] for s in stats]
berisso["dsm_median"] = [s["median"] for s in stats]
berisso["dsm_max"] = [s["max"] for s in stats]

berisso[
    [
        "fna",
        "dsm_min",
        "dsm_mean",
        "dsm_median",
        "dsm_max",
    ]
]
```

Zonal statistics are not limited to continuous data, either. Below, we aggregate the Mapbiomas data to assess relative composition of Berisso's land cover by category.

```{python}
# Extract raster values and remove NoData
values = suelo_2022_masked.values.flatten()
values = values[~np.isnan(values)]

# Count pixels of each unique ID
unique_ids, counts = np.unique(values, return_counts=True)

# Group IDs by coverage category
counts_by_category = {}
for name, info in categories.items():
    total = sum(
        counts[unique_ids == pixel_id][0] if pixel_id in unique_ids else 0
        for pixel_id in info["ids"]
    )
    if total > 0:
        counts_by_category[name] = {"count": total, "color": info["color"]}

# Calculate percentages
total_pixels = sum(cat["count"] for cat in counts_by_category.values())
for name in counts_by_category:
    percentage = (counts_by_category[name]["count"] / total_pixels) * 100
    counts_by_category[name]["percentage"] = percentage

# Sort from highest to lowest percentage
sorted_categories = sorted(
    counts_by_category.items(), key=lambda x: x[1]["percentage"], reverse=True
)
```

```{python}
#| code-fold: true
#| code-summary: "Show chart formatting code"

# Create horizontal bar chart
fig, ax = plt.subplots()

names = [cat[0] for cat in sorted_categories]
percentages = [cat[1]["percentage"] for cat in sorted_categories]
colors = [cat[1]["color"] for cat in sorted_categories]

bars = ax.barh(names, percentages, color=colors, edgecolor="black")



ax.set_xlabel("Percentage (%)", fontsize=12)
ax.set_ylabel("Cover Type", fontsize=12)
ax.set_title(
    "Land Cover Distribution - Berisso (MapBiomas 2022)",
    fontsize=14,
    fontweight="bold",
)
ax.grid(True, alpha=0.3, axis="x")

# Add percentage labels on the bars
for bar, pct in zip(bars, percentages):
    ax.text(
        pct + 0.5,
        bar.get_y() + bar.get_height() / 2,
        f"{pct:.1f}%",
        va="center",
        fontsize=10,
    )

plt.tight_layout()
plt.show()
```

Throughout this chapter, we've demonstrated several fundamental spatial operations, such as reprojection, zonal statistics, spatial clipping, and spatial aggregation. These operations are central to most climate risk workflows, but there are, of course, plenty of other useful ways to process spatial information. You can learn more about these in the [additional resources](#additional-resources) provided below, and we'll also cover several of them in subsequent chapters and in the [cookbooks section](/en/cookbooks/) that follows these four core conceptual chapters.

By now, you have a basic understanding of what GIS is and how to work with geospatial data in Python. In the following chapter, we'll introduce you to the concept of open data; talk about some key sources of open data at local, national, and international levels; and show examples of accessing open datasets with Python tools via WFS/WMS, API, and in cloud object storage.

## Additional Resources

To deepen your knowledge of geospatial analysis and data science with Python, we recommend the following open access resources.

First, much of this chapter is based on the [Introduction to Geospatial Raster and Vector Data with Python](https://carpentries-incubator.github.io/geospatial-python/index.html) by The Carpentries. This is an excellent starting point for making sure you're comfortable with the basics.

If you're looking for a more comprehensive resource, [Geocomputation with Python](https://py.geocompx.org/) is an excellent, thorough book on reproducible geographic data analysis in Python and covers many of the above topics in much greater detail.

Lastly, for an in-depth look at geospatial data science and topics like spatial autorrelation, point pattern analysis, clustering, and spatial regression, we recommend [Geographic Data Science with Python](https://geographicdata.science/book/intro.html)
