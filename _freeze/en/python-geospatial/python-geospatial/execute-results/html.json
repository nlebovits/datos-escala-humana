{
  "hash": "861e99ac205f07b6e25ec4b180750bda",
  "result": {
    "engine": "jupyter",
    "markdown": "## Geospatial Python\n\n_This lesson is largely based on material from [Introduction to Geospatial Raster and Vector Data with Python](https://carpentries-incubator.github.io/geospatial-python/index.html) by The Carpentries, available under a [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license._\n\n### Objectives\n\nThis chapter develops basic GIS programming skills for working with open data on climate risk. You will learn about geospatial data types, projections, spatial operations, basic analysis, and visualization.\n\n### What is GIS?\n\n**GIS (Geographic Information System)** is software that allows you to interact with geospatial data: capture, store, analyze, and visualize information about locations on the Earth's surface. These tools are fundamental for understanding spatial patterns, making informed decisions about resources and territory, and communicating geographic information effectively.\n\nIn the context of climate risk management, GIS makes it possible to do things such as identify areas vulnerable to flooding, analyze urban temperature patterns, evaluate green infrastructure coverage, and much more. Common tools include desktop applications (ArcGIS, QGIS), spatial databases (PostGIS), processing libraries (GDAL), cloud platforms (Google Earth Engine), and programming languages (R and Python).\n\nThere are two main approaches for working with geospatial data:\n\n - **Graphical user interface (GUI)** applications like ArcGIS and QGIS allow you to work with spatial data without learning a programming language. Mapping and visualization are more intuitive and flexible in this environment. However, these tools have important limitations: low reproducibility of analyses, limited scalability for automation, restricted capacity to customize functionality, and complex interfaces with too many buttons that can be overwhelming. \n - On the other hand, **programming** offers high reproducibility, scalability, and complete automation. It provides total flexibility to customize functionality and facilitates integration with other workflows and data sources. The main disadvantages are the need to learn a programming language, a steeper learning curve at the beginning, and that mapping and visualization are less intuitive initially.\n\nIn this book, we use **Python** for all of our analyses. Python is widely used with a rich ecosystem of specialized libraries for geospatial analysis. It is flexible, powerful, and relatively easy to learn compared to other languages. It offers complete integration with databases, APIs, web services, and other programming languages. Additionally, it is free, open source, and has an active community that provides abundant documentation, tutorials, and support. Lastly, its high replicability makes it ideal for sharing analyses that can be adapted and scaled in other contexts.\n\n#### Cloud-Native Geospatial\n**Cloud-native geospatial** tools are designed specifically to work with data stored in the cloud. Unlike traditional tools that require downloading complete datasets, these tools allow you to access and process only the portions of data needed through protocols like HTTP range requests.\n\n::: {.column-margin}\nThe **cloud** basically means someone else’s computer: servers that your device connects to over the internet. Cloud computing offers power, flexibility, and scalability as needed without requiring you to configure local servers. It provides distributed and reliable infrastructure. However, it has an initial learning curve, costs can accumulate with intensive use, and it requires constant internet connection.\n:::\n\nExamples include Cloud Optimized GeoTIFF (COG) for raster data, which allows efficient streaming of satellite images without downloading complete files. GeoParquet and FlatGeobuf are cloud-optimized vector formats that support fast spatial queries. Platforms like Google Earth Engine, Microsoft Planetary Computer, and AWS Earth Search provide massive catalogs of satellite data with APIs for analysis at scale.\n\nThese tools are especially relevant for climate risk analysis, where we need to process large volumes of satellite, climate, and geographic data without the infrastructure to store them locally. The cloud-native tools we will use are not more difficult than traditional tools and offer many advantages. The cloud is simply one of the places where people compute today. As mentioned in [the introduction](../intro.qmd#guiding-principles), this book takes the cloud-native approach as the default.\n\n### Coordinate Reference Systems (CRS)\n\n[Coordinate reference systems](https://py.geocompx.org/01-spatial-data#sec-coordinate-reference-systems-intro) define how the spatial elements of data relate to the Earth's surface. CRS are geographic or projected. [Geographic systems](https://py.geocompx.org/01-spatial-data#geographic-coordinate-systems) identify locations using longitude and latitude in decimal degrees on a spherical or ellipsoidal surface. [Projected systems](https://py.geocompx.org/01-spatial-data#sec-projected-coordinate-reference-systems) convert the three-dimensional surface of the Earth into Cartesian coordinates (x, y) in meters on an implicitly flat surface.\n\n![Examples of different CRS, Opennews.org via [@carpentries_geospatial_python]](../../public/python-geoespacial/us-crs-versions.jpg)\n\nAll projection introduces deformations. Therefore, some properties of the Earth's surface are distorted: area, direction, distance, and shape. A projection can preserve only one or two of these properties. Projections are named according to the property they preserve: equal-area preserves area, azimuthal preserves direction, equidistant preserves distance, and conformal preserves local shape.\n\nIf you imagine that the Earth is an orange, the way you peel it and then flatten the skin is similar to how projections are made.\n\n![Orange metaphor [@carpentries_geospatial_python]](../../public/python-geoespacial/orange-peel-earth.jpg)\n\n::: {.callout-tip}\nWhen problems arise with spatial analysis, the cause is often CRS-related issues. When troubleshooting, one of the first things you should check is whether there is a CRS problem.\n:::\n\n## Vector data\n\nThe [vector data model](https://py.geocompx.org/01-spatial-data#sec-vector-data) represents geographic features with points, lines, and polygons. These geometries have discrete and well-defined boundaries, which means that vector data generally has high precision. Points can represent independent features (like the location of a bus stop) or they can connect to form more complex geometries like lines and polygons.\n\n![Vector data [@carpentries_geospatial_python]](../../public/python-geoespacial/spatial_extent.png)\n\nVector data is organized into a table of attributes and geometries. The [geometry column](https://py.geocompx.org/01-spatial-data#sec-geometry-columns) is essential in a GeoDataFrame: it contains the geometric part of the vector layer and is the basis for all spatial operations. This column can contain point, line, polygon, or multipolygon, and also stores spatial reference information (CRS).\n\n![GeoDataframe [@carpentries_geospatial_python]](../../public/python-geoespacial/pandas_geopandas_relation.png)\n\nVectors dominate the social sciences because human settlements and processes (like transportation infrastructure) tend to have discrete boundaries.\n\n### Python libraries for vector data\n\n[GeoPandas](https://geopandas.org/) extends Pandas to work with spatial geometries using [Shapely](https://shapely.readthedocs.io/) for geometric operations and [Fiona](https://fiona.readthedocs.io/) for file reading/writing. We also use [Matplotlib](https://matplotlib.org/) for basic graphics and visualization.\n\n### Vector file formats\n\nFor cloud-native workflows, GeoParquet is a columnar format optimized for analytical queries of large vector datasets, and FlatGeobuf provides efficient spatial access through built-in spatial indexing. Both support HTTP range requests, meaning you can query subsets of data without downloading entire files.\n\nYou'll also encounter GeoJSON (.geojson, .json), which is used for web mapping and stores coordinates as text using JavaScript Object Notation. GeoPackage (.gpkg) is a single-file format that works well for local data exchange. ESRI Shapefile (.shp, .dbf, .shx) is a legacy format that requires multiple files and has technical limitations (field name length, attribute types, file size), but you'll still encounter it in the wild because many organizations haven't migrated yet.\n\nWe load municipal boundaries for Buenos Aires Province from ARBA's WFS service [@arba_partidos_pba]. The code below checks for a cached local copy first, then fetches from the WFS service if needed.\n\n::: {#3667578c .cell execution_count=1}\n``` {.python .cell-code}\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport os\nfrom owslib.wfs import WebFeatureService\nfrom io import StringIO, BytesIO\n\npartidos_path = \"../../public/python-geoespacial/pba_partidos.parquet\"\n\n# Load partidos from ARBA GeoServer WFS service\nif os.path.exists(partidos_path):\n    partidos = gpd.read_parquet(partidos_path)\nelse:\n    # Connect to WFS (Web Feature Service)\n    wfs_url = \"https://geo.arba.gov.ar/geoserver/idera/wfs\"\n    wfs = WebFeatureService(url=wfs_url, version=\"2.0.0\")\n\n    # Request the Departamento layer (partidos)\n    response = wfs.getfeature(\n        typename=\"idera:Departamento\",\n        srsname=\"EPSG:5347\",  # Original CRS\n    )\n\n    # Convert WFS response to GeoDataFrame\n    partidos = gpd.read_file(BytesIO(response.read()))\n\n    # Save to cache for future use\n    partidos.to_parquet(partidos_path)\n\n# Reproject to working CRS\npartidos = partidos.to_crs(\"EPSG:5348\")  # POSGAR 2007 / Argentina 4\n```\n:::\n\n\n### Reprojections\n\nTransforming data from one coordinate system to another is frequently necessary when working with multiple data sources. All data must be in the same CRS before performing spatial analysis. In the code above, we reprojected the partidos data to EPSG:5348 (POSGAR 2007 / Argentina 4) using GeoPandas' `to_crs()` method. This ensures our data is in a projected coordinate system with units in meters, which is appropriate for spatial calculations in the La Plata region.\n\n::: {.callout-tip}\nWhen you have datasets in different coordinate systems, choose a projected CRS that covers your study area. For Argentina, POSGAR 2007 divides the country into 7 Gauss-Krueger zones. Use the zone that covers your municipality. If your study area spans multiple zones, choose the zone that covers the majority of your area. You can confirm CRS codes at [epsg.io](https://epsg.io).\n:::\n\n::: {#3c8f0e7d .cell execution_count=2}\n``` {.python .cell-code}\npartidos.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gml_id</th>\n      <th>cca</th>\n      <th>cde</th>\n      <th>fna</th>\n      <th>gna</th>\n      <th>nam</th>\n      <th>sag</th>\n      <th>ara3</th>\n      <th>arl</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Departamento.437</td>\n      <td>065</td>\n      <td>06505</td>\n      <td>Partido de Magdalena</td>\n      <td>Partido</td>\n      <td>Magdalena</td>\n      <td>ARBA</td>\n      <td>1849.53</td>\n      <td>1785.29</td>\n      <td>MULTIPOLYGON (((6435320.257 6134850.321, 64352...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Departamento.434</td>\n      <td>090</td>\n      <td>06686</td>\n      <td>Partido de Rojas</td>\n      <td>Partido</td>\n      <td>Rojas</td>\n      <td>ARBA</td>\n      <td>2060.72</td>\n      <td>1978.87</td>\n      <td>MULTIPOLYGON (((6128472.001 6214278.169, 61285...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Departamento.436</td>\n      <td>058</td>\n      <td>06455</td>\n      <td>Partido de Las Flores</td>\n      <td>Partido</td>\n      <td>Las Flores</td>\n      <td>ARBA</td>\n      <td>3341.36</td>\n      <td>3340.27</td>\n      <td>MULTIPOLYGON (((6292198.682 5974257.073, 62916...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Departamento.485</td>\n      <td>031</td>\n      <td>06266</td>\n      <td>Partido de Exaltación de la Cruz</td>\n      <td>Partido</td>\n      <td>Exaltación de la Cruz</td>\n      <td>ARBA</td>\n      <td>636.36</td>\n      <td>634.17</td>\n      <td>MULTIPOLYGON (((6286757.47 6218024.36, 6286781...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Departamento.435</td>\n      <td>046</td>\n      <td>06364</td>\n      <td>Partido de General Rodríguez</td>\n      <td>Partido</td>\n      <td>General Rodríguez</td>\n      <td>ARBA</td>\n      <td>365.40</td>\n      <td>360.14</td>\n      <td>MULTIPOLYGON (((6327496.29 6167316.466, 632748...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#0fc1aac8 .cell execution_count=3}\n``` {.python .cell-code}\npartidos.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](python-geospatial_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n### Filter and manipulate geometries\n\nThe `partidos` data has several attributes: department code, department name, and geometry. To extract only the data for the Partido de La Plata, we filter for the observation where the \"fna\" column corresponds to \"Partido de La Plata\".\n\n::: {#e12accae .cell execution_count=4}\n``` {.python .cell-code}\nla_plata = partidos[partidos[\"fna\"] == \"Partido de La Plata\"]\n\nla_plata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gml_id</th>\n      <th>cca</th>\n      <th>cde</th>\n      <th>fna</th>\n      <th>gna</th>\n      <th>nam</th>\n      <th>sag</th>\n      <th>ara3</th>\n      <th>arl</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>Departamento.496</td>\n      <td>055</td>\n      <td>06441</td>\n      <td>Partido de La Plata</td>\n      <td>Partido</td>\n      <td>La Plata</td>\n      <td>ARBA</td>\n      <td>892.63</td>\n      <td>942.23</td>\n      <td>MULTIPOLYGON (((6395811.772 6128078.545, 63970...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWhen we map it we see that it includes not only the urban area but also an island that is technically part of the partido.\n\n::: {#37488830 .cell execution_count=5}\n``` {.python .cell-code}\nla_plata.plot(figsize=(12, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](python-geospatial_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nWhen we examine the geometry type, we see that we have a multipolygon. The geometry can be a point, a line, a polygon, or a multipolygon.\n\n::: {#8eb5f3d8 .cell execution_count=6}\n``` {.python .cell-code}\nprint(la_plata.geometry.iloc[0].geom_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMultiPolygon\n```\n:::\n:::\n\n\nWe want to remove the island because it does not correspond to our analysis. We extract the larger geometry of the two.\n\n::: {#994bdee4 .cell execution_count=7}\n``` {.python .cell-code}\n# Get the main geometry and keep only the largest polygon\nla_plata = la_plata.copy()\nmain_geom = la_plata.geometry.iloc[0]\nla_plata.loc[la_plata.index[0], \"geometry\"] = max(main_geom.geoms, key=lambda p: p.area)\n```\n:::\n\n\nWhen we check the geometry again, we see that it is now a simple polygon and only the partido de La Plata appears without the island.\n\n::: {#88b6a806 .cell execution_count=8}\n``` {.python .cell-code}\nprint(la_plata.geometry.iloc[0].geom_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPolygon\n```\n:::\n:::\n\n\n::: {#d57a2c3a .cell execution_count=9}\n``` {.python .cell-code}\nla_plata.plot(figsize=(12, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](python-geospatial_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nThe GeoDataFrame class extends the pandas DataFrame, allowing you to treat non-spatial attributes like a table and process them with standard methods. Spatial predicates are geometric relationships between objects: intersection, containment, proximity, etc. These operations are fundamental for analyses like identifying neighborhoods within flood zones or finding critical infrastructure near risk areas. For more details, see [Geocomputation with Python, Chapter 1](https://py.geocompx.org/01-spatial-data#sec-vector-data).\n\n## Raster data\n\nThe [raster data model](https://py.geocompx.org/01-spatial-data#sec-raster-data) represents the world through a continuous grid of cells of constant size. This model consists of metadata (header) that defines the coordinate system, the origin, and the resolution, plus a matrix of values. The origin is typically the coordinate of the lower-left corner. This representation avoids explicitly storing the coordinates of each cell, making raster processing much more efficient than vector processing.\n\n![Raster data [@carpentries_geospatial_python]](../../public/python-geoespacial/raster_concept.png)\n\nCell values are numeric, representing continuous variables (elevation, temperature, precipitation) or categorical variables (land cover types, use classes). Rasters usually represent continuous phenomena, although discrete features like soil classes can also be represented in this model. However, the edges of discrete features can become blurry in raster datasets.\n\n::: {.column-margin}\n**Continuous data** refer to values that can take any number within a range (including decimals). These values are stored as floats (floating-point numbers) instead of integers, allowing you to represent precise measurements like 15.7 meters or 23.42 meters of elevation. **Categorical data** represents categories by encoding them as integers.\n:::\n\nRasters dominate many environmental sciences due to dependence on satellite remote sensing data. Examples include satellite images, aerial photographs, digital elevation models, and precipitation maps. In this chapter we use digital elevation models (continuous) and land cover data (categorical).\n\n### Python libraries for raster data\n\n[Rasterio](https://rasterio.readthedocs.io/) reads and writes georeferenced raster formats. [Xarray](https://xarray.dev/) and [rioxarray](https://corteva.github.io/rioxarray/) facilitate working with labeled multidimensional arrays, ideal for climate time series. [GDAL/OGR](https://gdal.org/) are the fundamental libraries that underlie many tools.\n\n### Raster file formats\n\nGeoTIFF (.tif, .tiff) is the industry standard format for GIS applications and satellite remote sensing. Cloud Optimized GeoTIFF (COG) is an optimized variant that incorporates tiles and previews to support HTTP queries, allowing you to load subsets of the image without transferring the complete file. For cloud-native workflows with multidimensional data, Zarr is a format optimized for cloud storage, ideal for climate time series.\n\nWe work with 30m resolution Digital Surface Model data from the Copernicus DEM [@copernicus_dem_openlandmap], accessed via the Element84 Earth Search STAC catalog. We use Cloud Optimized GeoTIFF along with the `rioxarray` module to import only the necessary data efficiently. Instead of importing all the global data (several terabytes), we import only our area of interest through lazy loading. This means we don't download the data until we have filtered it, thus using only the essential memory and computational power.\n\n::: {#f77ff720 .cell execution_count=10}\n``` {.python .cell-code}\nimport pystac_client\nfrom odc.stac import load\nimport rioxarray as rio\n\n# Reproject to geographic CRS for bounding box query\naoi = la_plata.to_crs(\"EPSG:4326\")\n\n# Configure AWS for unsigned requests (no login required)\nos.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\nos.environ[\"GDAL_DISABLE_READDIR_ON_OPEN\"] = \"EMPTY_DIR\"\n\n# Connect to Element84 Earth Search STAC catalog\ncatalog = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\n\n# Get bounding box for our area of interest\n# bbox format: [min_x, min_y, max_x, max_y]\nbbox = aoi.total_bounds\n\n# Search for Copernicus DEM tiles covering our area\n# cop-dem-glo-30 is the global 30m elevation dataset\nsearch = catalog.search(\n    collections=[\"cop-dem-glo-30\"],\n    bbox=bbox,\n)\n\n# Get the items (files) that cover our area\nitems = list(search.items())\n\n# Load DSM using chunks for efficient memory processing\n# odc.stac.load automatically downloads and assembles the necessary tiles\ndsm = load(\n    items,\n    bbox=bbox,\n    chunks={\"x\": 1024, \"y\": 1024},  # Process in blocks for efficiency\n)\n\n# Extract elevation band and convert to DataArray for use with rioxarray\n# load() returns a Dataset; we need the elevation DataArray\ndsm = dsm[\"data\"].squeeze()  # Remove extra dimensions\n\n# Clip raster to exact geometries\ndsm_clipped = dsm.rio.clip(aoi.geometry)\n\n# Reproject clipped area to working CRS (POSGAR 2007 / Argentina 4)\n# This is more efficient because we reproject only the area of interest\ndsm_reproj = dsm_clipped.rio.reproject(dst_crs=\"EPSG:5348\")\n```\n:::\n\n\nWe visualize the digital surface model for La Plata. The `robust=True` parameter improves contrast by ignoring extreme values, and `cmap=\"terrain\"` uses an appropriate color palette for elevation.\n\n::: {#4f748cd0 .cell execution_count=11}\n``` {.python .cell-code}\ndsm_reproj.plot(robust=True, cmap=\"terrain\", figsize=(12, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](python-geospatial_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n### Zonal statistics: connecting raster and vector\n\nZonal statistics allow you to calculate aggregate values from a raster within zones defined by vector polygons. This operation is fundamental for connecting continuous data (raster) with administrative units (vector). We extract elevation statistics for the La Plata area.\n\n::: {#5c11867c .cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\nfrom rasterstats import zonal_stats\n\nla_plata = la_plata.copy()\n\n# Extract elevation statistics (min, mean, median, max)\nstats = zonal_stats(\n    la_plata,\n    dsm_reproj.squeeze().values,\n    affine=dsm_reproj.rio.transform(),\n    stats=[\"min\", \"mean\", \"median\", \"max\"],\n    nodata=np.nan,\n)\n\n# Add the statistics as new columns\nla_plata[\"dsm_min\"] = [s[\"min\"] for s in stats]\nla_plata[\"dsm_mean\"] = [s[\"mean\"] for s in stats]\nla_plata[\"dsm_median\"] = [s[\"median\"] for s in stats]\nla_plata[\"dsm_max\"] = [s[\"max\"] for s in stats]\n\nla_plata[\n    [\n        \"fna\",\n        \"dsm_min\",\n        \"dsm_mean\",\n        \"dsm_median\",\n        \"dsm_max\",\n    ]\n]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fna</th>\n      <th>dsm_min</th>\n      <th>dsm_mean</th>\n      <th>dsm_median</th>\n      <th>dsm_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>Partido de La Plata</td>\n      <td>-4.320292</td>\n      <td>19.391156</td>\n      <td>20.339926</td>\n      <td>52.470333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe import 30m resolution categorical land cover data from MapBiomas Argentina Collection 1 [@mapbiomas_argentina_2022], covering 1998-2022. This is the most reliable land cover data currently available for Argentina. The dataset is accessed directly from MapBiomas' public Google Cloud Storage bucket without requiring a local copy, since we clip to our area of interest before downloading.\n\n::: {#48655a04 .cell execution_count=13}\n``` {.python .cell-code}\nsuelo_2022_ruta = \"https://storage.googleapis.com/mapbiomas-public/initiatives/argentina/collection-1/coverage/argentina_coverage_2022.tif\"\n\n# Open the raster with rioxarray and chunks\nsuelo_2022 = rio.open_rasterio(\n    suelo_2022_ruta,\n    chunks={\"x\": 4096, \"y\": 4096},\n)\n\n# Clip in the raster's native CRS (EPSG:4326)\nsuelo_2022_clipped = suelo_2022.rio.clip(\n    aoi.geometry.values,\n    from_disk=True,\n)\n\n# Reproject to working CRS (POSGAR 2007 / Argentina 4)\nsuelo_2022 = suelo_2022_clipped.rio.reproject(dst_crs=\"EPSG:5348\")\n\n# Mask NoData values\nsuelo_2022_masked = suelo_2022.where(\n    (suelo_2022 != suelo_2022.rio.nodata) & (suelo_2022 != 255)\n)\n```\n:::\n\n\n### Spatial clipping\n\nExtracting data within a specific area of interest is one of the most common spatial operations. In the code above, we clipped the national land cover data to the boundaries of La Plata using the `.rio.clip()` method. This operation reduces the dataset from the entire country to just our study area, making subsequent processing faster and more focused. Note that we clip in the raster's native CRS (EPSG:4326) before reprojecting to our working CRS (EPSG:5348).\n\n::: {#f165f510 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show legend and formatting code\"}\nimport matplotlib.colors as mcolors\n\n# Define land cover categories with their IDs and MapBiomas colors\ncategories = {\n    \"Closed woody\": {\"ids\": [3], \"color\": \"#1f8d49\"},\n    \"Open woody\": {\"ids\": [4], \"color\": \"#7dc975\"},\n    \"Sparse woody\": {\"ids\": [45], \"color\": \"#807a40\"},\n    \"Flooded woody\": {\"ids\": [6], \"color\": \"#026975\"},\n    \"Flooded non-woody vegetation\": {\"ids\": [11], \"color\": \"#519799\"},\n    \"Grassland\": {\"ids\": [12], \"color\": \"#d6bc74\"},\n    \"Steppe\": {\"ids\": [63], \"color\": \"#ebf8b5\"},\n    \"Pasture\": {\"ids\": [15], \"color\": \"#edde8e\"},\n    \"Agriculture\": {\"ids\": [18], \"color\": \"#e974ed\"},\n    \"Forest plantation\": {\"ids\": [9], \"color\": \"#7a5900\"},\n    \"Shrub crop\": {\"ids\": [36], \"color\": \"#d082de\"},\n    \"Agricultural mosaic\": {\"ids\": [21], \"color\": \"#ffefc3\"},\n    \"Non-vegetated area\": {\"ids\": [22], \"color\": \"#d4271e\"},\n    \"River, lake or ocean\": {\"ids\": [33], \"color\": \"#2532e4\"},\n    \"Ice and surface snow\": {\"ids\": [34], \"color\": \"#93dfe6\"},\n    \"Not observed\": {\"ids\": [27], \"color\": \"#ffffff\"},\n}\n\n# Get the unique IDs present in the data\nunique_values = np.unique(\n    suelo_2022_masked.values[~np.isnan(suelo_2022_masked.values)]\n).astype(int)\npresent_ids = set(unique_values)\n\n# Filter only the categories that appear in our data\npresent_categories = {\n    name: info\n    for name, info in categories.items()\n    if any(id in present_ids for id in info[\"ids\"])\n}\n\n# Create color map\ncolors_by_id = {}\nfor info in categories.values():\n    for pixel_id in info[\"ids\"]:\n        colors_by_id[pixel_id] = info[\"color\"]\n\nsorted_ids = sorted(colors_by_id.keys())\ncmap = mcolors.ListedColormap([colors_by_id[i] for i in sorted_ids])\nnorm = mcolors.BoundaryNorm(sorted_ids, cmap.N)\n\n# Visualize the land cover raster\nfig, ax = plt.subplots(figsize=(20, 20))\nsuelo_2022_masked.plot(ax=ax, cmap=cmap, norm=norm, add_colorbar=False)\n\n# Create legend only with the present categories\nlegend = [\n    plt.Rectangle((0, 0), 1, 1, facecolor=info[\"color\"], label=name)\n    for name, info in present_categories.items()\n]\n\nax.legend(\n    handles=legend,\n    loc=\"lower center\",\n    bbox_to_anchor=(0.5, -0.1),\n    ncol=3,\n    frameon=False,\n)\n\nplt.title(\"Land Cover - MapBiomas 2022\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](python-geospatial_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n### Analyze territorial distribution\n\nTo better understand the composition of the territory, we calculate what percentage of the La Plata area corresponds to each type of land cover.\n\n::: {#32c53bc3 .cell execution_count=15}\n``` {.python .cell-code}\n# Extract raster values and remove NoData\nvalues = suelo_2022_masked.values.flatten()\nvalues = values[~np.isnan(values)]\n\n# Count pixels of each unique ID\nunique_ids, counts = np.unique(values, return_counts=True)\n\n# Group IDs by coverage category\ncounts_by_category = {}\nfor name, info in categories.items():\n    total = sum(\n        counts[unique_ids == pixel_id][0] if pixel_id in unique_ids else 0\n        for pixel_id in info[\"ids\"]\n    )\n    if total > 0:\n        counts_by_category[name] = {\"count\": total, \"color\": info[\"color\"]}\n\n# Calculate percentages\ntotal_pixels = sum(cat[\"count\"] for cat in counts_by_category.values())\nfor name in counts_by_category:\n    percentage = (counts_by_category[name][\"count\"] / total_pixels) * 100\n    counts_by_category[name][\"percentage\"] = percentage\n\n# Sort from highest to lowest percentage\nsorted_categories = sorted(\n    counts_by_category.items(), key=lambda x: x[1][\"percentage\"], reverse=True\n)\n```\n:::\n\n\n::: {#3186c202 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show chart formatting code\"}\n# Create horizontal bar chart\nfig, ax = plt.subplots(figsize=(12, 8))\n\nnames = [cat[0] for cat in sorted_categories]\npercentages = [cat[1][\"percentage\"] for cat in sorted_categories]\ncolors = [cat[1][\"color\"] for cat in sorted_categories]\n\nbars = ax.barh(names, percentages, color=colors, edgecolor=\"black\")\n\n\n\nax.set_xlabel(\"Percentage (%)\", fontsize=12)\nax.set_ylabel(\"Cover Type\", fontsize=12)\nax.set_title(\n    \"Land Cover Distribution - La Plata (MapBiomas 2022)\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nax.grid(True, alpha=0.3, axis=\"x\")\n\n# Add percentage labels on the bars\nfor bar, pct in zip(bars, percentages):\n    ax.text(\n        pct + 0.5,\n        bar.get_y() + bar.get_height() / 2,\n        f\"{pct:.1f}%\",\n        va=\"center\",\n        fontsize=10,\n    )\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](python-geospatial_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\n### Spatial aggregation\n\nSpatial aggregation combines multiple geometries or calculates summary statistics within geographic areas. We demonstrate this by counting building footprints within La Plata using cloud-native building data from the Google-Microsoft-OSM Open Buildings dataset [@google_microsoft_osm_buildings].\n\n::: {#2bba3f35 .cell execution_count=17}\n``` {.python .cell-code}\nimport duckdb\nimport pandas as pd\n\n# Get bounding box of La Plata in WGS84 (EPSG:4326) for cloud query\nbounds_4326 = la_plata.to_crs(\"EPSG:4326\").total_bounds\n\n# Configure DuckDB with spatial and network extensions\ncon = duckdb.connect()\ncon.execute(\"INSTALL spatial\")\ncon.execute(\"LOAD spatial\")\ncon.execute(\"INSTALL httpfs\")\ncon.execute(\"LOAD httpfs\")\n\n# Configure DuckDB for S3 cloud storage access\ncon.execute(\"SET s3_region='us-east-1'\")\ncon.execute(\"SET s3_endpoint='data.source.coop'\")\ncon.execute(\"SET s3_use_ssl=true\")\ncon.execute(\"SET s3_url_style='path'\")\n\n# Query buildings within our bounding box and save to temporary file\n# This filters buildings using geographic bounds (bbox) to load only those in our area of interest\ntemp_file = \"buildings_filtered.parquet\"\nquery = f\"\"\"\nCOPY (\n    SELECT *\n    FROM 's3://vida/google-microsoft-open-buildings/geoparquet/by_country/country_iso=ARG/ARG.parquet'\n    WHERE bbox.xmax >= {bounds_4326[0]} AND bbox.xmin <= {bounds_4326[2]} AND\n          bbox.ymax >= {bounds_4326[1]} AND bbox.ymin <= {bounds_4326[3]}\n) TO '{temp_file}' (FORMAT PARQUET);\n\"\"\"\ncon.execute(query)\n\n# Read filtered building data from Parquet file\nbuildings_df = pd.read_parquet(temp_file)\n\n# Deserialize geometries: convert from binary format (WKB - Well-Known Binary) to GeoSeries objects\n# GeoParquet stores geometries as bytes in WKB format, we need to convert them to GeoPandas geometries\nbuildings_df[\"geometry\"] = gpd.GeoSeries.from_wkb(buildings_df[\"geometry\"])\n\n# Create GeoDataFrame with deserialized geometries\n# Specify original CRS as EPSG:4326 (WGS84) since data comes in that coordinate system\nbuildings = gpd.GeoDataFrame(buildings_df, geometry=\"geometry\", crs=\"EPSG:4326\")\n\n# Reproject to working CRS to match other analysis data\nbuildings = buildings.to_crs(\"EPSG:5348\")\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"e67575cab9be4dddbaa9ba47286152d7\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n:::\n\n\nNow we spatially filter buildings that fall within La Plata's boundaries and count them:\n\n::: {#48c4b31a .cell execution_count=18}\n``` {.python .cell-code}\n# Spatial join: keep only buildings that intersect with La Plata polygon\nbuildings_in_la_plata = gpd.sjoin(buildings, la_plata, predicate=\"intersects\", how=\"inner\")\n\n# Count total buildings\ntotal_buildings = len(buildings_in_la_plata)\n\nprint(f\"Total buildings in La Plata: {total_buildings:,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal buildings in La Plata: 625,936\n```\n:::\n:::\n\n\n::: {#e62d597e .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show map visualization code\"}\n# Visualize buildings within La Plata\nfig, ax = plt.subplots(figsize=(12, 12))\n\n# Plot La Plata boundary\nla_plata.plot(ax=ax, facecolor=\"none\", edgecolor=\"black\", linewidth=2)\n\n# Plot buildings with orange outlines\nbuildings_in_la_plata.plot(ax=ax, facecolor=\"none\", edgecolor=\"orange\", linewidth=0.5)\n\nax.set_title(f\"Building Footprints in La Plata\\nTotal buildings: {total_buildings:,}\",\n             fontsize=14, fontweight=\"bold\")\nax.set_xticks([])\nax.set_yticks([])\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.spines[\"bottom\"].set_visible(False)\nax.spines[\"left\"].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](python-geospatial_files/figure-html/cell-20-output-1.png){}\n:::\n:::\n\n\n## Other Common Spatial Operations\n\nThroughout this chapter, we've demonstrated several fundamental spatial operations: reprojections (converting between coordinate systems), spatial clipping (extracting data for a specific area), zonal statistics (summarizing raster values within vector polygons), and spatial aggregation (combining geometries by attributes). These operations form the core of most climate risk workflows.\n\nOther common spatial operations you'll encounter in geospatial analysis include spatial overlays (combining multiple data layers based on spatial relationships) and buffers (creating zones around features for proximity analysis). These operations are covered in detail in the resources below and will appear in later chapters when we apply them to specific climate risk problems.\n\n## Additional Resources\n\nTo deepen your knowledge of geospatial analysis with Python, consult these complete and free resources:\n\nRey, S., Arribas-Bel, D., & Wolf, L. (2023). [Geographic Data Science with Python](https://geographicdata.science/book/intro.html). CRC Press.\n\nDorman, M., Graser, A., Nowosad, J., & Lovelace, R. (2025). [Geocomputation with Python](https://py.geocompx.org/). CRC Press.\n\nTo explore traditional GIS software and commercial platforms, visit [The Carpentries Geospatial Python workshop](https://carpentries-incubator.github.io/geospatial-python/index.html) which includes detailed information about open source options (QGIS, GRASS GIS, GDAL) and commercial solutions (ArcGIS, MapInfo).\n\n",
    "supporting": [
      "python-geospatial_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"5712919e86544d02a17c5ae0edc5780b\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":\"black\",\"description_width\":\"\"}},\"c6b6419162e140afad083a3891131c34\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":\"auto\"}},\"e67575cab9be4dddbaa9ba47286152d7\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_c6b6419162e140afad083a3891131c34\",\"max\":100,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_5712919e86544d02a17c5ae0edc5780b\",\"tabbable\":null,\"tooltip\":null,\"value\":100}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}